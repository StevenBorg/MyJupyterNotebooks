{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports are excessive, but some are used when moving to Azure Functions later\n",
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import azure.functions as func\n",
    "import azure.storage.queue as asq\n",
    "import azure.storage.blob as blob\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import httpx\n",
    "import asyncio\n",
    "import time\n",
    "import azure.storage.blob.aio as blobaio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the series in all of TCIA\n",
    "df = pd.read_csv('C:\\\\githealth\\\\data\\\\TciaSeriesDataForAllCollections.csv')\n",
    "\n",
    "# Discard a bunch of columns, keeping only the below\n",
    "df = df[['Collection', 'StudyInstanceUID', 'SeriesCount', 'SeriesInstanceUID', 'ImageCount']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of data - best to just choose one approach\n",
    "\n",
    "# Option 1:  Select only a few series from each Collection to take a sample\n",
    "#df_samples = df.groupby('Collection').nth([0,1,2,3,4]) # Grabs first 5 rows\n",
    "#len(df_samples)\n",
    "\n",
    "# Option 2: Select only a single Collection.  First print them, so you can pick which ones you want.\n",
    "cols = df.Collection.unique()\n",
    "print(cols)\n",
    "collection_name = \"ACRIN-FLT-Breast\" # replace with desired collection\n",
    "df_samples = df[df['Collection'] == collection_name] \n",
    "len(df_samples)\n",
    "\n",
    "\n",
    "# Option 3: Do whatever you like\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of SeriesInstanceUIDs to iterate over\n",
    "series_list = df_samples['SeriesInstanceUID'].tolist()\n",
    "len(series_list)\n",
    "\n",
    "#series_list = series_list[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload with httpx - an async library - plus with async blob uploads\n",
    "In this code, each series zip file download is done serially, but the blob uploads are done in parallel, sometimes hundreds at a time. The reason for this is the code is meant to execute inside an Azure Function, with a single function call being responsible for the instance downloads (via zip) of a single series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few variables\n",
    "\n",
    "# Use one of the two ways below to define the container name. This code currently only uploads to a single container.\n",
    "#container_name = 'samples' \n",
    "container_name = collection_name.lower() # containers only take lowercase names\n",
    "storageConnString = 'DefaultEndpointsProtocol=https;AccountName=tciadicoms;AccountKey=+uHbmJGRcox0FPfqW24iv1JhzycVoleE/iC9ThfT8UjfqSkwnbk5LjKGOeA2vkEKuMgShCfUbt9u9uE36gtWwA==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "print(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up a connection to the storage container. Ensure storageConnString is set up above\n",
    "\n",
    "blob_container_sync = blob.ContainerClient.from_connection_string(conn_str=storageConnString,container_name=container_name)\n",
    "blob_container = blobaio.ContainerClient.from_connection_string(conn_str=storageConnString,container_name=container_name)\n",
    "\n",
    "# Create the blob store container if it doesn't exist...  by exception\n",
    "#   Which is hacky, but effective\n",
    "try:\n",
    "    await blob_container.get_container_properties()\n",
    "except:\n",
    "    await blob_container.create_container()\n",
    "    \n",
    "\n",
    "# Get list of all series already downloaded in storage container\n",
    "#  This gets a list of blob metadata, takes the name, splits it on / to get the folder name (series) using [0]\n",
    "\n",
    "# Only do this if you want to avoid re-uploading any series. But beware, if only a single file in a series is uploaded\n",
    "#   the series will show up in the items below\n",
    "series_blobs = [bl.name.split('/')[0] for bl in blob_container_sync.list_blobs()]\n",
    "series_blobs = list(set(series_blobs)) # Converting a list to set removes all duplicates but need to make it a list again\n",
    "len(series_blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_container.container_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload a single file async, throw exception on failure to be caught elsewhere\n",
    "async def uploadSingleFileToBlob(file_name, file_bytes, blob_container):\n",
    "    print(f'uploading {file_name}')    \n",
    "    try:\n",
    "        return await blob_container.upload_blob(data=file_bytes, name=file_name)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(e.ErrorCode)\n",
    "        #raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do retries of the upload.\n",
    "async def uploadSingleFileToBlobWithRetries(file_name, file_bytes, blob_container):\n",
    "    print(f'Uploading {file_name}')\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            stored_blob = await uploadSingleFileToBlob(file_bytes = file_bytes, file_name=file_name, blob_container=blob_container)\n",
    "            print(f'Blob stored as: {stored_blob}')\n",
    "            return stored_blob\n",
    "        except:\n",
    "            #there was an error, try again\n",
    "            print(f'sleeping {i}')\n",
    "            #time.sleep(i) #sleep increasing amount of time and try again.\n",
    "            continue\n",
    "    \n",
    "    return None # at this point, we've failed, but no sense throwing exception for testung data use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an async upload.\n",
    "async def uploadSeriesInstancesToBlob(series_id, async_client, blob_container):\n",
    "    urlGetImageBase = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getImage'\n",
    "    params = {'SeriesInstanceUID': series_id}\n",
    "\n",
    "    # Not using streaming, since it looks like perf is higher with a full download then upload async to blob\n",
    "    #  But does use a lot more memory\n",
    "    \n",
    "    try:\n",
    "        r = await async_client.get(urlGetImageBase,params=params,timeout=None) #timeout=15.0)\n",
    "\n",
    "        file_like_object = BytesIO(r.content)\n",
    "\n",
    "        z = zipfile.ZipFile(file_like_object)\n",
    "        \n",
    "        print(f'Files in zip: {len(z.filelist)}')\n",
    "\n",
    "        # Using a loop here instead of a list comprehension. Much slower, but can be understood easily\n",
    "        files_to_upload = [] # array to hold filenames and byte arrays\n",
    "\n",
    "        for f in z.filelist:\n",
    "            dicom_file = z.read(f) #this is the byte array\n",
    "            parts = f.filename.split('/')\n",
    "            dcm_parts = [p for p in parts if p.find('.dcm') != -1] # split the name and get filename (could also use Path, but whatevs)\n",
    "            if len(dcm_parts) == 1: # we have a dicom file, and only one\n",
    "                dcm_name = f'{series_id}/{dcm_parts[0]}' #this is the filename (series/fname)\n",
    "                #print(dcm_name)\n",
    "                #uploadSingleFileToBlob(dcm_name, dicom_file, blob_container)\n",
    "                #print(up)\n",
    "                files_to_upload.append((dcm_name,dicom_file))\n",
    "\n",
    "        # We now have a list of (filename,filebytes) tuples\n",
    "        # Upload them all asynchronously.  This might be insane.  Might want to batch...  Even small series can have hundreds of files\n",
    "        #   but it seems to be working...  at least for the few items I've tried\n",
    "        print('About to call upload on zip files')\n",
    "        responses = await asyncio.gather(*[uploadSingleFileToBlobWithRetries(fname, fbytes, blob_container) for fname, fbytes in files_to_upload])\n",
    "      \n",
    "        return responses\n",
    "    except:\n",
    "        #there was an error, return None - avoiding exceptions for now...\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload a whole collection, but only one series at a time.\n",
    "#   Ideally replace this with a call to an Azure Function to trigger the download\n",
    "\n",
    "series_list = df_samples['SeriesInstanceUID'].tolist()\n",
    "print(len(series_list))\n",
    "\n",
    "# get an async client to reuse. \n",
    "async_client = httpx.AsyncClient()\n",
    "\n",
    "print(f'Starting upload of {len(series_list)} series from {collection_name}')\n",
    "for series in series_list:\n",
    "    if series in series_blobs: # already uploaded at least one instance to blob\n",
    "        print(f'  Already uploaded {series}')\n",
    "        continue \n",
    "    print(f' Uploading series: {series}')\n",
    "    start = time.time()\n",
    "    uploaded_blobs = await uploadSeriesInstancesToBlob(series, async_client, blob_container)\n",
    "    duration = time.time() - start\n",
    "    if uploaded_blobs is not None:\n",
    "        print(f'Uploaded {len(uploaded_blobs)} instances from series uid: {series} in {duration}')\n",
    "    else:\n",
    "        print('None returned!  Problem!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageConnString = 'DefaultEndpointsProtocol=https;AccountName=sjbfunctest;AccountKey=XuYBliYrXazCmfDdK2jLcaJcfqPgu8tC43TlltTMY413nusjx2N6+IvErYmVXuZfOBVgVaCQ52RObKioS9FDRg==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "series_queue = asq.QueueClient.from_connection_string(conn_str=storageConnString,queue_name='series')\n",
    "\n",
    "# Create the queue if it doesn't exist...  by exception\n",
    "#   Which is hacky, but effective\n",
    "try:\n",
    "    series_queue.get_queue_properties()\n",
    "except:\n",
    "    series_queue.create_queue()\n",
    "\n",
    "# Must base-64 encode since... functions...\n",
    "enc = asq.TextBase64EncodePolicy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following uploads a single series, use for troubleshooting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload a single file. Ignore this cell if you're uploading a whole collection\n",
    "series_list = df_samples['SeriesInstanceUID'].tolist()\n",
    "print(len(series_list))\n",
    "print(series_list[0])\n",
    "async_client = httpx.AsyncClient()\n",
    "#uploaded_blobs = await uploadSeriesInstancesToBlob(series_list[3], async_client, blob_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following uploads using httpx - an asynchronous library\n",
    "\n",
    "# Please skip this one and use the one with parallel blob uploads. It is much faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an async upload.\n",
    "\n",
    "# Don't use this one.  Just for reference, as it doesn't have the parallel uploads of blobs.\n",
    "\n",
    "async def uploadSeriesInstancesToBlob(series_id, async_client, blob_container):\n",
    "    urlGetImageBase = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getImage'\n",
    "    params = {'SeriesInstanceUID': series_id}\n",
    "\n",
    "    # Not using streaming, since it looks like perf is higher with a full download then upload async to blob\n",
    "    #  But does use a lot more memory\n",
    "    \n",
    "    try:\n",
    "        r = await async_client.get(urlGetImageBase,params=params,timeout=None) #timeout=15.0)\n",
    "\n",
    "        #with httpx.get(urlGetImage,params=params,timeout=None, stream=True) as r:\n",
    "\n",
    "        file_like_object = BytesIO(r.content)\n",
    "\n",
    "        z = zipfile.ZipFile(file_like_object)\n",
    "        files = z.filelist\n",
    "        print(files)\n",
    "\n",
    "        for f in files:\n",
    "            dicom_file = z.read(f)\n",
    "            parts = f.filename.split('/')\n",
    "            dcm_parts = [p for p in parts if p.find('.dcm') != -1]\n",
    "            if len(dcm_parts) == 1: # we have a dicom file, and only one\n",
    "                dcm_name = f'{series_id}/{dcm_parts[0]}'\n",
    "                print(dcm_name)\n",
    "                up = blob_container.upload_blob(data=z.read(f), name=dcm_name)\n",
    "                #print(up)\n",
    "\n",
    "        return True\n",
    "    except:\n",
    "        #there was an error, return False\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following uploads using requests - a synchronous library\n",
    "\n",
    "# Please skip this one and use the Async one above, it is many times faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Upload using requests - a synchronous library\n",
    "\n",
    "storageConnString = 'DefaultEndpointsProtocol=https;AccountName=tciadicoms;AccountKey=t216FXH+Q1FpcE746SgxGRwkZ6qEXiw2vu1J40kUtT2phZvJ5usCppiDQp5YW9NQcWGwIsS+eJBrOFDIORrbyg==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "b = blob.ContainerClient.from_connection_string(conn_str=storageConnString,container_name='samples')\n",
    "\n",
    "# Create the blob store container if it doesn't exist...  by exception\n",
    "#   Which is hacky, but effective\n",
    "try:\n",
    "    b.get_container_properties()\n",
    "except:\n",
    "    b.create_container()\n",
    "    \n",
    "    \n",
    "# Get list of all series already downloaded in storage container\n",
    "#  This gets a list of blob metadata, takes the name, splits it on / to get the folder name (series) using [0]\n",
    "series_blobs = [bl.name.split('/')[0] for bl in b.list_blobs()]\n",
    "series_blobs = list(set(series_blobs)) # Converting a list to set removes all duplicates but need to make it a list again\n",
    "\n",
    "for series_id in series_list:\n",
    "    print(series_id)\n",
    "    \n",
    "    # if the series has been downloaded (or even started) then move on.\n",
    "    if series_id in series_blobs:\n",
    "        print(f'Already uploaded {series_id}. Skipping.')\n",
    "        continue\n",
    "    \n",
    "    if series_id in ['1.2.840.113713.4.2.20015998712384536720405088092843300246','1.2.840.113713.4.2.277260943311504281313806422270862087705','1.3.6.1.4.1.14519.5.2.1.4429.7055.273220878682060174312749196256']:\n",
    "        print(f'No way! {series_id} is bad. Skipping.')\n",
    "        continue\n",
    "    \n",
    "    #get a single series (likely with multiple instances) and upload to blob\n",
    "    urlGetImage = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getImage'\n",
    "    params = {'SeriesInstanceUID': series_id}\n",
    "    #res = requests.get(urlGetImage,params=params,timeout=None) #timeout=15.0)\n",
    "    with requests.get(urlGetImage,params=params,timeout=None, stream=True) as r:\n",
    "\n",
    "        file_like_object = BytesIO(r.content)\n",
    "\n",
    "        z = zipfile.ZipFile(file_like_object)\n",
    "        files = z.filelist\n",
    "        print(f'Uploading {len(files)} files')\n",
    "\n",
    "        for f in files:\n",
    "            dicom_file = z.read(f)\n",
    "            parts = f.filename.split('/')\n",
    "            dcm_parts = [p for p in parts if p.find('.dcm') != -1]\n",
    "            if len(dcm_parts) == 1: # we have a dicom file, and only one\n",
    "                dcm_name = f'{series_id}/{dcm_parts[0]}'\n",
    "                print(dcm_name)\n",
    "                up = b.upload_blob(data=z.read(f), name=dcm_name)\n",
    "                #print(up)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
