{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  This works with HTTPX\n",
    "async def foo():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        url = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollectionValues'\n",
    "        r = await client.get(url) #'https://www.example.com/')\n",
    "    return r\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    await asyncio.gather(*[foo() for x in range(i)])\n",
    "    print(f'{i} runs in {time.time() - start} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call TCIA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import azure.storage.queue as asq\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# variables used throughout\n",
    "tciabase = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleRateLimits(headers):\n",
    "    limits = dict([(h,int(headers[h])) for h in headers if h.find('x-rate') != -1])\n",
    "    # Example : {'x-ratelimit-limit-hour': 360000, 'x-ratelimit-remaining-hour': 359961, 'x-ratelimit-limit-second': 1000, 'x-ratelimit-remaining-second': 999}\n",
    "    \n",
    "    # TODO: send a queue message to a queue which will 'pause' the querying from TCIA for the right time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getCollectionsAsync():\n",
    "    urlGetCollections = f'{tciabase}/getCollectionValues'\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        r = await client.get(urlGetCollections)\n",
    "    \n",
    "    # not really needed here, but should send every time we send a request\n",
    "    handleRateLimits(r.headers)\n",
    "    \n",
    "    if r is not None:\n",
    "        #return [c['Collection'] for c in r.json()]\n",
    "        return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tcia_collections = await getCollectionsAsync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(tcia_collections)\n",
    "#print(tcia_collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://services.cancerimagingarchive.net/services/v3/TCIA/query/getPatientStudy?Collection=TCGA-GBM\n",
    "# https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSeries?Collection=TCGA-GBM&StudyInstanceUID=1.3.6.1.4.1.14519.5.2.1.7695.4001.130563880911723253267280582465  \n",
    "# https://services.cancerimagingarchive.net/services/v3/TCIA/query/getImage?SeriesInstanceUID=1.3.6.1.4.1.14519.5.2.1.7695.4001.306204232344341694648035234440\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get PatientStudies in Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getPatientStudiesPerCollection(collection, async_client):\n",
    "    urlGetPatientStudyBase = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getPatientStudy'\n",
    "    params = {'Collection': collection}\n",
    "    #async with httpx.AsyncClient() as client:\n",
    "    r = await async_client.get(urlGetPatientStudyBase,params=params,timeout=None) #timeout=15.0)\n",
    "    \n",
    "    # not really needed here, but should send every time we send a request\n",
    "    handleRateLimits(r.headers)\n",
    "    \n",
    "    if r is not None:\n",
    "        #return [c['Collection'] for c in r.json()]\n",
    "        return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#studies = await getPatientStudiesPerCollection(collections[0]['Collection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(studies))\n",
    "#print(studies[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#study_ids = [s['StudyInstanceUID'] for s in studies] # if s.find('StudyInstanceUID') != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getSeriesPerStudy(study, async_client):\n",
    "    urlGetSeriesBase = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSeries'\n",
    "    params = {'StudyInstanceUID': study}\n",
    "    #async with httpx.AsyncClient() as client:\n",
    "    r = await async_client.get(urlGetSeriesBase,params=params,timeout=None) #timeout=15.0)\n",
    "    \n",
    "    # not really needed here, but should send every time we send a request\n",
    "    handleRateLimits(r.headers)\n",
    "    \n",
    "    if r is not None:\n",
    "        #return [c['Collection'] for c in r.json()]\n",
    "        return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#series = await getSeriesPerStudy(studies[0]['StudyInstanceUID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(series)\n",
    "#print(series[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getSeriesPerStudy(study, async_client):\n",
    "    urlGetSeriesBase = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSeries'\n",
    "    params = {'StudyInstanceUID': study}\n",
    "    #async with httpx.AsyncClient() as client:\n",
    "    r = await async_client.get(urlGetSeriesBase,params=params,timeout=None) #timeout=15.0)\n",
    "    \n",
    "    # not really needed here, but should send every time we send a request\n",
    "    handleRateLimits(r.headers)\n",
    "    \n",
    "    if r is not None:\n",
    "        #return [c['Collection'] for c in r.json()]\n",
    "        return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getSeriesPerStudyJoinDict(study_id, study_dict, async_client):\n",
    "    \n",
    "    #retry three times if necessary\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            series = await getSeriesPerStudy(study_id, async_client)\n",
    "            # join with the study_dict we already have\n",
    "                    #create a list of series dicts (combining metadata from study)\n",
    "            # merge the dictionaries using ** to unpack the dictionaries (since .union is in place)\n",
    "            series_list = [] # list to store all the series for a collection\n",
    "\n",
    "            for s in series:\n",
    "                merged_dict = {**study_dict, **s}\n",
    "                series_list.append(merged_dict)        \n",
    "            return series_list\n",
    "        except:\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "    \n",
    "    # if we get here we have a problem that's happened three times\n",
    "    raise Exception(\"Failed to get series over set of retries\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getInstancesPerSeries(series, async_client):\n",
    "    urlGetInstancesBase = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSOPInstanceUIDs'\n",
    "    params = {'SeriesInstanceUID': series}\n",
    "    #async with httpx.AsyncClient() as client:\n",
    "    r = await async_client.get(urlGetInstancesBase,params=params,timeout=None) #timeout=15.0)\n",
    "    \n",
    "    # not really needed here, but should send every time we send a request\n",
    "    handleRateLimits(r.headers)\n",
    "    \n",
    "    if r is not None:\n",
    "        #return [c['Collection'] for c in r.json()]\n",
    "        return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getInstancesPerSeriesJoinDict(series_id, series_dict, async_client):\n",
    "    \n",
    "    #retry, if necessary\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            instances = await getInstancesPerSeries(series_id, async_client)\n",
    "            # join with the study_dict we already have\n",
    "                    #create a list of series dicts (combining metadata from study)\n",
    "            # merge the dictionaries using ** to unpack the dictionaries (since .union is in place)\n",
    "            instances_list = [] # list to store all the series for a collection\n",
    "\n",
    "            for s in instances:\n",
    "                merged_dict = {**series_dict, **s}\n",
    "                instances_list.append(merged_dict)   \n",
    "            return instances_list\n",
    "        except:\n",
    "            time.sleep(i)\n",
    "            continue\n",
    "    \n",
    "    # if we get here we have a problem that's happened three times\n",
    "    raise Exception(\"Failed to get series over set of retries\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the collections. We'll operate on one at a time\n",
    "\n",
    "tcia_collections = await getCollectionsAsync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Collection': 'CPTAC-CM'}, {'Collection': 'NaF PROSTATE'}, {'Collection': 'NSCLC-Radiomics-Interobserver1'}, {'Collection': 'NSCLC Radiogenomics'}, {'Collection': 'PROSTATE-DIAGNOSIS'}, {'Collection': 'RIDER Lung PET-CT'}, {'Collection': 'TCGA-GBM'}, {'Collection': 'LGG-1p19qDeletion'}, {'Collection': 'QIN GBM Treatment Response'}, {'Collection': 'TCGA-READ'}, {'Collection': 'TCGA-LUSC'}, {'Collection': 'APOLLO'}, {'Collection': 'TCGA-KIRP'}, {'Collection': 'QIN-HEADNECK'}, {'Collection': 'CT COLONOGRAPHY'}, {'Collection': 'BREAST-DIAGNOSIS'}, {'Collection': 'PROSTATE-MRI'}, {'Collection': 'RIDER PHANTOM MRI'}, {'Collection': 'LungCT-Diagnosis'}, {'Collection': 'QIN-BRAIN-DSC-MRI'}, {'Collection': 'TCGA-LUAD'}, {'Collection': 'CBIS-DDSM'}, {'Collection': 'CPTAC-LSCC'}, {'Collection': 'LCTSC'}, {'Collection': 'RIDER Breast MRI'}, {'Collection': 'RIDER PHANTOM PET-CT'}, {'Collection': 'NSCLC-Radiomics'}, {'Collection': 'MRI-DIR'}, {'Collection': 'QIN LUNG CT'}, {'Collection': 'QIBA CT-1C'}, {'Collection': 'CPTAC-SAR'}, {'Collection': 'LIDC-IDRI'}, {'Collection': 'Lung Phantom'}, {'Collection': 'Prostate Fused-MRI-Pathology'}, {'Collection': 'Brain-Tumor-Progression'}, {'Collection': 'Phantom FDA'}, {'Collection': 'HNSCC-3DCT-RT'}, {'Collection': 'TCGA-THCA'}, {'Collection': 'ACRIN-NSCLC-FDG-PET'}, {'Collection': 'ACRIN-FLT-Breast'}, {'Collection': 'TCGA-STAD'}, {'Collection': 'TCGA-HNSC'}, {'Collection': 'CPTAC-GBM'}, {'Collection': 'Lung-Fused-CT-Pathology'}, {'Collection': 'LDCT-and-Projection-data'}, {'Collection': 'C4KC-KiTS'}, {'Collection': '4D-Lung'}, {'Collection': 'DRO-Toolkit'}, {'Collection': 'PROSTATEx'}, {'Collection': 'Pancreas-CT'}, {'Collection': 'CPTAC-PDA'}, {'Collection': 'TCGA-ESCA'}, {'Collection': 'TCGA-CESC'}, {'Collection': 'Head-Neck Cetuximab'}, {'Collection': 'CPTAC-CCRCC'}, {'Collection': 'ISPY1'}, {'Collection': 'TCGA-BLCA'}, {'Collection': 'CC-Radiomics-Phantom-3'}, {'Collection': 'IvyGAP'}, {'Collection': 'QIN Breast DCE-MRI'}, {'Collection': 'REMBRANDT'}, {'Collection': 'VICTRE'}, {'Collection': 'TCGA-PRAD'}, {'Collection': 'CPTAC-LUAD'}, {'Collection': 'Anti-PD-1_MELANOMA'}, {'Collection': 'RIDER NEURO MRI'}, {'Collection': 'ACRIN-FMISO-Brain'}, {'Collection': 'HNSCC'}, {'Collection': 'CC-Radiomics-Phantom-2'}, {'Collection': 'QIN PET Phantom'}, {'Collection': 'HEAD-NECK-RADIOMICS-HN1'}, {'Collection': 'QIN-BREAST'}, {'Collection': 'OPC-Radiomics'}, {'Collection': 'TCGA-UCEC'}, {'Collection': 'Pelvic-Reference-Data'}, {'Collection': 'NSCLC-Radiomics-Genomics'}, {'Collection': 'CC-Radiomics-Phantom'}, {'Collection': 'TCGA-KICH'}, {'Collection': 'Mouse-Mammary'}, {'Collection': 'TCGA-KIRC'}, {'Collection': 'AAPM-RT-MAC'}, {'Collection': 'Prostate-3T'}, {'Collection': 'CT Lymph Nodes'}, {'Collection': 'TCGA-OV'}, {'Collection': 'TCGA-BRCA'}, {'Collection': 'Breast-MRI-NACT-Pilot'}, {'Collection': 'Anti-PD-1_Lung'}, {'Collection': 'RIDER Lung CT'}, {'Collection': 'SPIE-AAPM Lung CT Challenge'}, {'Collection': 'TCGA-LIHC'}, {'Collection': 'Head-Neck-PET-CT'}, {'Collection': 'Soft-tissue-Sarcoma'}, {'Collection': 'CPTAC-HNSCC'}, {'Collection': 'PDMR-BL0293-F563'}, {'Collection': 'TCGA-SARC'}, {'Collection': 'Mouse-Astrocytoma'}, {'Collection': 'TCGA-LGG'}, {'Collection': 'TCGA-COAD'}, {'Collection': 'CPTAC-UCEC'}, {'Collection': 'PDMR-292921-168-R'}, {'Collection': 'Lung-PET-CT-Dx'}, {'Collection': 'COVID-19-AR'}, {'Collection': 'ACRIN-DSC-MR-Brain'}, {'Collection': 'Prostate-MRI-US-Biopsy'}, {'Collection': 'QIN-PROSTATE-Repeatability'}]\n"
     ]
    }
   ],
   "source": [
    "print(tcia_collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already processed CPTAC-CM. Please check the data folder.\n",
      "Already processed NaF PROSTATE. Please check the data folder.\n",
      "Already processed NSCLC-Radiomics-Interobserver1. Please check the data folder.\n",
      "Already processed NSCLC Radiogenomics. Please check the data folder.\n",
      "Already processed PROSTATE-DIAGNOSIS. Please check the data folder.\n",
      "Already processed RIDER Lung PET-CT. Please check the data folder.\n",
      "Already processed TCGA-GBM. Please check the data folder.\n",
      "Already processed LGG-1p19qDeletion. Please check the data folder.\n",
      "Already processed QIN GBM Treatment Response. Please check the data folder.\n",
      "Already processed TCGA-READ. Please check the data folder.\n",
      "Already processed TCGA-LUSC. Please check the data folder.\n",
      "Already processed APOLLO. Please check the data folder.\n",
      "Already processed TCGA-KIRP. Please check the data folder.\n",
      "Already processed QIN-HEADNECK. Please check the data folder.\n",
      "Already processed CT COLONOGRAPHY. Please check the data folder.\n",
      "Already processed BREAST-DIAGNOSIS. Please check the data folder.\n",
      "Already processed PROSTATE-MRI. Please check the data folder.\n",
      "Already processed RIDER PHANTOM MRI. Please check the data folder.\n",
      "Already processed LungCT-Diagnosis. Please check the data folder.\n",
      "Already processed QIN-BRAIN-DSC-MRI. Please check the data folder.\n",
      "Already processed TCGA-LUAD. Please check the data folder.\n",
      "Already processed CBIS-DDSM. Please check the data folder.\n",
      "Already processed CPTAC-LSCC. Please check the data folder.\n",
      "Already processed LCTSC. Please check the data folder.\n",
      "Already processed RIDER Breast MRI. Please check the data folder.\n",
      "Already processed RIDER PHANTOM PET-CT. Please check the data folder.\n",
      "Already processed NSCLC-Radiomics. Please check the data folder.\n",
      "Already processed MRI-DIR. Please check the data folder.\n",
      "Already processed QIN LUNG CT. Please check the data folder.\n",
      "Already processed QIBA CT-1C. Please check the data folder.\n",
      "Already processed CPTAC-SAR. Please check the data folder.\n",
      "Already processed LIDC-IDRI. Please check the data folder.\n",
      "Already processed Lung Phantom. Please check the data folder.\n",
      "Already processed Prostate Fused-MRI-Pathology. Please check the data folder.\n",
      "Already processed Brain-Tumor-Progression. Please check the data folder.\n",
      "Already processed Phantom FDA. Please check the data folder.\n",
      "Already processed HNSCC-3DCT-RT. Please check the data folder.\n",
      "Already processed TCGA-THCA. Please check the data folder.\n",
      "Already processed ACRIN-NSCLC-FDG-PET. Please check the data folder.\n",
      "Already processed ACRIN-FLT-Breast. Please check the data folder.\n",
      "Already processed TCGA-STAD. Please check the data folder.\n",
      "Already processed TCGA-HNSC. Please check the data folder.\n",
      "Already processed CPTAC-GBM. Please check the data folder.\n",
      "Already processed Lung-Fused-CT-Pathology. Please check the data folder.\n",
      "Already processed LDCT-and-Projection-data. Please check the data folder.\n",
      "Already processed C4KC-KiTS. Please check the data folder.\n",
      "Already processed 4D-Lung. Please check the data folder.\n",
      "Already processed DRO-Toolkit. Please check the data folder.\n",
      "Already processed PROSTATEx. Please check the data folder.\n",
      "Already processed Pancreas-CT. Please check the data folder.\n",
      "Already processed CPTAC-PDA. Please check the data folder.\n",
      "Already processed TCGA-ESCA. Please check the data folder.\n",
      "Already processed TCGA-CESC. Please check the data folder.\n",
      "Already processed Head-Neck Cetuximab. Please check the data folder.\n",
      "Already processed CPTAC-CCRCC. Please check the data folder.\n",
      "Already processed ISPY1. Please check the data folder.\n",
      "Already processed TCGA-BLCA. Please check the data folder.\n",
      "Already processed CC-Radiomics-Phantom-3. Please check the data folder.\n",
      "Already processed IvyGAP. Please check the data folder.\n",
      "Already processed QIN Breast DCE-MRI. Please check the data folder.\n",
      "Already processed REMBRANDT. Please check the data folder.\n",
      "Already processed VICTRE. Please check the data folder.\n",
      "Already processed TCGA-PRAD. Please check the data folder.\n",
      "Already processed CPTAC-LUAD. Please check the data folder.\n",
      "Already processed Anti-PD-1_MELANOMA. Please check the data folder.\n",
      "Already processed RIDER NEURO MRI. Please check the data folder.\n",
      "Already processed ACRIN-FMISO-Brain. Please check the data folder.\n",
      "Already processed HNSCC. Please check the data folder.\n",
      "Already processed CC-Radiomics-Phantom-2. Please check the data folder.\n",
      "Already processed QIN PET Phantom. Please check the data folder.\n",
      "Already processed HEAD-NECK-RADIOMICS-HN1. Please check the data folder.\n",
      "Already processed QIN-BREAST. Please check the data folder.\n",
      "Already processed OPC-Radiomics. Please check the data folder.\n",
      "Already processed TCGA-UCEC. Please check the data folder.\n",
      "Already processed Pelvic-Reference-Data. Please check the data folder.\n",
      "Already processed NSCLC-Radiomics-Genomics. Please check the data folder.\n",
      "Already processed CC-Radiomics-Phantom. Please check the data folder.\n",
      "Already processed TCGA-KICH. Please check the data folder.\n",
      "Already processed Mouse-Mammary. Please check the data folder.\n",
      "Already processed TCGA-KIRC. Please check the data folder.\n",
      "Already processed AAPM-RT-MAC. Please check the data folder.\n",
      "Already processed Prostate-3T. Please check the data folder.\n",
      "Already processed CT Lymph Nodes. Please check the data folder.\n",
      "Already processed TCGA-OV. Please check the data folder.\n",
      "Already processed TCGA-BRCA. Please check the data folder.\n",
      "Already processed Breast-MRI-NACT-Pilot. Please check the data folder.\n",
      "Already processed Anti-PD-1_Lung. Please check the data folder.\n",
      "Already processed RIDER Lung CT. Please check the data folder.\n",
      "Already processed SPIE-AAPM Lung CT Challenge. Please check the data folder.\n",
      "Already processed TCGA-LIHC. Please check the data folder.\n",
      "Already processed Head-Neck-PET-CT. Please check the data folder.\n",
      "Already processed Soft-tissue-Sarcoma. Please check the data folder.\n",
      "Already processed CPTAC-HNSCC. Please check the data folder.\n",
      "Already processed PDMR-BL0293-F563. Please check the data folder.\n",
      "Already processed TCGA-SARC. Please check the data folder.\n",
      "Already processed Mouse-Astrocytoma. Please check the data folder.\n",
      "Already processed TCGA-LGG. Please check the data folder.\n",
      "Already processed TCGA-COAD. Please check the data folder.\n",
      "Already processed CPTAC-UCEC. Please check the data folder.\n",
      "Already processed PDMR-292921-168-R. Please check the data folder.\n",
      "Already processed Lung-PET-CT-Dx. Please check the data folder.\n",
      "Already processed COVID-19-AR. Please check the data folder.\n",
      "Already processed ACRIN-DSC-MR-Brain. Please check the data folder.\n",
      "Already processed Prostate-MRI-US-Biopsy. Please check the data folder.\n",
      "Already processed QIN-PROSTATE-Repeatability. Please check the data folder.\n"
     ]
    }
   ],
   "source": [
    "# For each collection, get the Studies, and then the Series\n",
    "# Hypothesis: pandas is far more memory efficient than Python dicts so first go get every single\n",
    "#   Study for all passed in collections and put into a DataFrame and THEN go get Series\n",
    "\n",
    "already_processed_count = 0\n",
    "for collection in tcia_collections: # iter through the dictionaries in the list\n",
    "    \n",
    "    # To save a LOT of time, don't rerun collections if they've already been run\n",
    "\n",
    "    if os.path.exists(f'data/{collection[\"Collection\"]}-series.json'):\n",
    "        print(f'Already processed {collection[\"Collection\"]}. Please check the data folder.')\n",
    "        already_processed_count +=1 \n",
    "        continue\n",
    "        \n",
    "    print(f'Already processed: {already_processed_count}')    \n",
    "    \n",
    "    #if collection['Collection'] in ['CBIS-DDSM','VICTRE','HNSCC']: #QIBA CT-1C only 486, others over 7,000, HNSCC is 1200\n",
    "    #    print(f'Skipping {collection[\"Collection\"]} because of size')\n",
    "    #    continue\n",
    "    \n",
    "    #NOTE: GBM-DSC-MRI-DRO has ZERO series????\n",
    "    \n",
    "    series_list = [] # list to store all the series for a collection\n",
    "    counter = 0\n",
    "    # Get the Studies\n",
    "    \n",
    "    #with httpx.AsyncClient() as async_client: # This doesn't work with 0.12.1 of HTTPX, must be explicit\n",
    "    async_client = httpx.AsyncClient()\n",
    "    print('getting studies')\n",
    "    studies = await getPatientStudiesPerCollection(collection['Collection'], async_client)\n",
    "    print(f'studies in {collection[\"Collection\"]} : {len(studies)}')\n",
    "    #print(f'{collection[\"Collection\"]}_studies.csv')\n",
    "    study_df = pd.DataFrame.from_dict(studies)\n",
    "    study_df.to_csv(f'data/{collection[\"Collection\"]}_studies.csv')\n",
    "\n",
    "    # Need to make this MUCH faster\n",
    "#    for study in studies[:3]:\n",
    "#        series = await getSeriesPerStudy(study['StudyInstanceUID'], async_client)\n",
    "#        counter += 1\n",
    "#        if counter % 50 == 0:\n",
    "#            print(str(counter))\n",
    "\n",
    "        #create a list of series dicts (combining metadata from study)\n",
    "#        for s in series:\n",
    "#            # merge the dictionaries using ** to unpack the dictionaries (since .union is in place)\n",
    "#            merged_dict = {**study, **s}\n",
    "#            series_list.append(merged_dict)\n",
    "\n",
    "    # Try this instead\n",
    "    study_ids = [s['StudyInstanceUID'] for s in studies]\n",
    "    batchsize = 10    \n",
    "\n",
    "\n",
    "    for i in range(0,len(study_ids),batchsize):\n",
    "        start = time.time()                  \n",
    "        trimmed_range = [y for y in range(i,i+batchsize) if y<len(study_ids)]   #only used to shorten the last batch    \n",
    "        print(trimmed_range)\n",
    "        #trimmed_study_ids = [study_ids[i] for i in trimmed_range]\n",
    "        #print(trimmed_study_ids)\n",
    "        trimmed_studies = [studies[i] for i in trimmed_range]\n",
    "        #print(trimmed_studies)\n",
    "        #1.3.6.1.4.1.14519.5.2.1.7695.4001.130563880911723253267280582465\n",
    "        responses = await asyncio.gather(*[getSeriesPerStudyJoinDict(s[\"StudyInstanceUID\"],s, async_client) for s in trimmed_studies])\n",
    "        #responses = await asyncio.gather(*[getSeriesPerStudyJoinDict('1.3.6.1.4.1.14519.5.2.1.7695.4001.130563880911723253267280582465',s, async_client) for s in trimmed_studies])\n",
    "        # unfortunately responses is a list of list of dicts, we need to flatten\n",
    "        flatten_matrix = [val for sublist in responses for val in sublist] \n",
    "        #print(len(flatten_matrix))\n",
    "        [series_list.append(r) for r in flatten_matrix]\n",
    "        #print(len(series_list))\n",
    "        print(f'Studies {i}-{i+batchsize} in {time.time() - start} seconds')  \n",
    "    #print(len(series_list))    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Output results to a data folder to avoid having to burn time running again\n",
    "    study_df = pd.DataFrame.from_dict(series_list)\n",
    "    study_df.to_csv(f'data/{collection[\"Collection\"]}_studies_series.csv')\n",
    "\n",
    "    # Also save just the resulting list, since that can be useful, too.  :-)\n",
    "    with open(f'data/{collection[\"Collection\"]}-series.json',\"w\") as f:\n",
    "        json.dump(series_list, f)\n",
    "    \n",
    "    # close the connection explicitly, until with is supported by HTTPX\n",
    "    await async_client.aclose()\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Series Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use existing TCIA collections\n",
    "for collection in tcia_collections: # iter through the dictionaries in the list\n",
    "    \n",
    "    # Don't try to do items that aready exist\n",
    "    if not os.path.exists(f'data/{collection[\"Collection\"]}-series.json'):\n",
    "        print(f'Cannot find csv for {collection[\"Collection\"]}. Please check the data folder.')\n",
    "        continue\n",
    "        \n",
    "    if collection['Collection']==\"LDCT-and-Projection-data\":\n",
    "        print('skipping LDCT and Projection data')\n",
    "        continue\n",
    "        \n",
    "    if os.path.exists(f'data/{collection[\"Collection\"]}-instances.json'):\n",
    "        print(f'Already processed {collection[\"Collection\"]}. Please check the data folder.')\n",
    "        continue\n",
    "        \n",
    "    print(f'Processing {collection[\"Collection\"]}.')\n",
    "    with open(f'data/{collection[\"Collection\"]}-series.json', 'r') as myfile:\n",
    "        data = myfile.read()\n",
    "\n",
    "    data = data.replace(\"\\'\", \"\\\"\") # stupidly encoded my JSON with single quotes, instead of double - fix that with hack\n",
    "    series=json.loads(data)\n",
    "    \n",
    "    async_client = httpx.AsyncClient()\n",
    " \n",
    "    instance_list = []    \n",
    "    #series_ids = [s['SeriesInstanceUID'] for s in series]\n",
    "    batchsize = 50    \n",
    "\n",
    "    #print(series_ids[:10])   \n",
    "    #series = series[:60]\n",
    "    print(f'Processing {len(series)} series.')\n",
    "\n",
    "    for i in range(0,len(series),batchsize):               \n",
    "        trimmed_range = [y for y in range(i,i+batchsize) if y<len(series)]   #only used to shorten the last batch    \n",
    "        #print(f'trimmed_range is {trimmed_range}')\n",
    "\n",
    "        trimmed_series = [series[i] for i in trimmed_range]\n",
    "        #print(len(trimmed_series))\n",
    "        start = time.time()   \n",
    "        \n",
    "      \n",
    "        responses = await asyncio.gather(*[getInstancesPerSeriesJoinDict(s[\"SeriesInstanceUID\"],s, async_client) for s in trimmed_series])\n",
    "        print(f'Series {i}-{i+batchsize} in {time.time() - start} seconds')\n",
    "        #print(len(responses))\n",
    "        # unfortunately responses is a list of list of dicts, we need to flatten\n",
    "        flatten_matrix = [val for sublist in responses for val in sublist] \n",
    "        #print(len(flatten_matrix))\n",
    "        [instance_list.append(r) for r in flatten_matrix]\n",
    "    \n",
    "    print(len(instance_list))\n",
    "    \n",
    "\n",
    "\n",
    "    # Output results to a data folder to avoid having to burn time running again\n",
    "    df = pd.DataFrame.from_dict(instance_list)\n",
    "    df.to_csv(f'data/{collection[\"Collection\"]}_studies_series_instances.csv')\n",
    "\n",
    "    # Also save just the resulting list, since that can be useful, too.  :-)\n",
    "    with open(f'data/{collection[\"Collection\"]}-instances.json',\"w\") as f:\n",
    "        json.dump(instance_list, f)\n",
    "    \n",
    "    # close the connection explicitly\n",
    "    await async_client.aclose()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(instance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(study_df))\n",
    "study_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in series_list[:1]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_list2 = list(set(series_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete\n",
    "study_ids = study_ids[0:2]\n",
    "\n",
    "for i in range(0,len(study_ids),batchsize):\n",
    "    start = time.time()                  \n",
    "    trimmed_range = [y for y in range(i,i+batchsize) if y<len(study_ids)]   #only used to shorten the last batch    \n",
    "    print(trimmed_range)\n",
    "    trimmed_study_ids = [study_ids[sid] for sid in trimmed_range]\n",
    "    print(trimmed_study_ids)\n",
    "\n",
    "    responses = await asyncio.gather(*[getSeriesPerStudyJoinDict(sid, async_client) for sid in trimmed_study_ids])\n",
    "    # unfortunately responses is a list of list of dicts, we need to flatten\n",
    "    flatten_matrix = [val for sublist in responses for val in sublist] \n",
    "    #print(len(flatten_matrix))\n",
    "    [series_list.append(r) for r in flatten_matrix]\n",
    "    print(len(series_list))\n",
    "    print(f'Studies {i}-{i+batchsize} in {time.time() - start} seconds')  \n",
    "#print(len(series_list))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    start = time.time()\n",
    "    await asyncio.gather(*[foo() for x in range(i)])\n",
    "    print(f'{i} runs in {time.time() - start} seconds')\n",
    "\n",
    "# Throttle requests to 50 concurrent\n",
    "semaphore = asyncio.Semaphore(50)\n",
    "#...\n",
    "async def limit_wrap(url):\n",
    "    async with semaphore:\n",
    "        # do what you want\n",
    "#...\n",
    "results = asyncio.gather([limit_wrap(url) for url in urls])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_ids = [s['StudyInstanceUID'] for s in studies]\n",
    "stuff = []\n",
    "\n",
    "l = [x for x in range(len(study_ids))]\n",
    "batchsize = 20\n",
    "for x in range(0,len(l),batchsize):\n",
    "    #print('--------')\n",
    "    trimmed = [y for y in range(x,x+batchsize) if y<len(l)]\n",
    "    print(trimmed)\n",
    "\n",
    "    for a in l[x:x+len(trimmed)]:\n",
    "        stuff.append(a)\n",
    "        \n",
    "print(len(stuff))\n",
    "print(len(study_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "study_ids = [s['StudyInstanceUID'] for s in studies]\n",
    "print(len(study_ids))\n",
    "\n",
    "\n",
    "series_list = [] # list to store all the series for a collection\n",
    "\n",
    "entire_start = time.time()\n",
    "async_client = httpx.AsyncClient()\n",
    "batchsize = 10\n",
    "batches=2\n",
    "for i in range(0,batchsize*batches,batchsize):\n",
    "    start = time.time()\n",
    "    responses = await asyncio.gather(*[getSeriesPerStudy(\"1.3.6.1.4.1.14519.5.2.1.7695.4001.130563880911723253267280582465\", async_client) for sid in study_ids[i:i+batchsize]])\n",
    "    # unfortunately responses is a list of list of dicts, we need to flatten\n",
    "    flatten_matrix = [val for sublist in responses for val in sublist] \n",
    "    print(len(flatten_matrix))\n",
    "    [series_list.append(r) for r in flatten_matrix]\n",
    "    print(len(series_list))\n",
    "    print(f'Studies {i}-{i+batchsize} in {time.time() - start} seconds')  \n",
    "await async_client.aclose()\n",
    "print(time.time() - entire_start)\n",
    "print(len(series_list))\n",
    "# 2 batches of 20 studies is 98 seconds, series len = 160\n",
    "# 2 batches of 10 studies is 50 seconds, series len = 80\n",
    "# 2 batches of  5 studius is 26 seconds, series len = 40\n",
    "# 2 batches of  2 studies is 22 seconds, series len = 16\n",
    "# 2 batches of  1 study   is 15 seconds, series len = 8\n",
    "# kind of linear growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(responses))\n",
    "print(len(series_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_matrix = [val for sublist in series_list for val in sublist] \n",
    "print(len(flatten_matrix))\n",
    "\n",
    "series_list[:2]\n",
    "df = pd.DataFrame.from_dict(series_list)\n",
    "print(len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make this MUCH faster\n",
    "\n",
    "\n",
    "\n",
    "for study in studies[:3]:\n",
    "    series = await getSeriesPerStudy(study['StudyInstanceUID'])\n",
    "\n",
    "    #create a list of series dicts (combining metadata from study)\n",
    "    for s in series:\n",
    "        # merge the dictionaries using ** to unpack the dictionaries (since .union is in place)\n",
    "        merged_dict = {**study, **s}\n",
    "        series_list.append(merged_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "collection = tcia_collections[1]\n",
    "with open(f'data/{collection[\"Collection\"]}-series-test.json',\"w\") as f:\n",
    "    json.dump(series_list, f)\n",
    "    #series_list = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(series_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list if we need to\n",
    "collection = tcia_collections[1]\n",
    "with open(f'data/{collection[\"Collection\"]}-series-test.json',\"r\") as f:\n",
    "    l = json.load(f)\n",
    "type(l) # should read 'list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = asq.QueueClient.from_connection_string(conn_str='DefaultEndpointsProtocol=https;AccountName=sjbfunctest;AccountKey=XuYBliYrXazCmfDdK2jLcaJcfqPgu8tC43TlltTMY413nusjx2N6+IvErYmVXuZfOBVgVaCQ52RObKioS9FDRg==;EndpointSuffix=core.windows.net', queue_name='foofoo3')\n",
    "\n",
    "try:\n",
    "    p = q.get_queue_properties()\n",
    "except:\n",
    "    q.create_queue()\n",
    "q.send_message('Hello-There ')\n",
    "r = asq.TextBase64EncodePolicy()\n",
    "r.encode('TEST-THIS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works with the addition of async with \n",
    "\n",
    "import httpx\n",
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "async def download(url:str):\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollections\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        resp = await client.get(url)\n",
    "    return resp\n",
    "\n",
    "async def download_lots(i):\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollections\"\n",
    "    await asyncio.gather(*[download(url) for x in range(i)])\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    asyncio.run(download_lots))  # used outside of Jupyter when I don't have an event loop\n",
    "\n",
    "for i in range(7):\n",
    "    start = time.time()\n",
    "    await download_lots(i)\n",
    "    print(f'{i} runs in {time.time() - start} seconds')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "import os\n",
    "\n",
    "async def download(url:str, folder:str):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    resp = await httpx.get(url)\n",
    "    resp.raise_for_status()\n",
    "    #async with aiofiles.open(os.path.join(folder, filename), \"wb\") as f:\n",
    "    #    await f.write(resp.content)\n",
    "\n",
    "\n",
    "async def download_all_photos(loops: str):\n",
    "    #resp = httpx.get(\"https://jsonplaceholder.typicode.com/photos\")\n",
    "    #resp.raise_for_status()\n",
    "    #urls = list(set(d[\"url\"] for d in resp.json()))[:10]\n",
    "    #os.makedirs(out_dir, exist_ok=True)\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollectionValues\"\n",
    "    await asyncio.gather(*[download(url, \"bob\") for x in range(loops)])\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    asyncio.run(download_all_photos('100_photos'))\n",
    "\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    await download_all_photos(i)\n",
    "    print(f'{i} runs in {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with httpx.AsyncClient() as client:\n",
    "    r = await client.get('https://www.example.com/')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  This works with HTTPX\n",
    "async def foo():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        url = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollectionValues'\n",
    "        r = await client.get(url) #'https://www.example.com/')\n",
    "    return r\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    await asyncio.gather(*[foo() for x in range(i)])\n",
    "    print(f'{i} runs in {time.time() - start} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "from aiohttp import ClientSession\n",
    "import asyncio\n",
    "\n",
    "async def call_url(x, session):\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSeries?Collection=TCGA-GBM&StudyInstanceUID=1.3.6.1.4.1.14519.5.2.1.7695.4001.130563880911723253267280582465\"\n",
    "    \n",
    "    response = await session.get(url, timeout=None)\n",
    "    response_json = await response.json()\n",
    "    return response_json\n",
    "\n",
    "\n",
    "async def run_program(x, session):\n",
    "    \"\"\"Wrapper for running program in an asynchronous manner\"\"\"\n",
    "    #try:\n",
    "    response = await call_url(x, session)\n",
    "        #print(f\"Response: {json.dumps(response, indent=2)}\")\n",
    "    #except Exception as err:\n",
    "        #print(f\"Exception occured: {err}\")\n",
    "        #pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    #async with httpx.AsyncClient() as session:\n",
    "    async with ClientSession as sesssion:\n",
    "        await asyncio.gather(*[run_program(x,session) for x in range(i)])\n",
    "   # print(f'{i} runs in {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    try:\n",
    "        response = await session.request(method='GET', url=url)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Response status ({url}): {response.status}\")\n",
    "    except HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An error ocurred: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works with aiohttp but not httpx \n",
    "import aiohttp\n",
    "import asyncio\n",
    "import time\n",
    "import httpx\n",
    "\n",
    "async def call_url(session):\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollections\"\n",
    "    #response = await session.request(method='GET', url=url)\n",
    "    response = await session.get(url=url)\n",
    "\n",
    "    return response\n",
    "\n",
    "for i in range(1,5):\n",
    "    start = time.time() # start time for timing event\n",
    "    async with aiohttp.ClientSession() as session: #use aiohttp\n",
    "    #async with httpx.AsyncClient as session:  #use httpx\n",
    "        await asyncio.gather(*[call_url(session) for x in range(i)])\n",
    "    print(f'{i} call(s) in {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import time\n",
    "import httpx\n",
    "\n",
    "async def call_url(session):\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollectionValues\"\n",
    "    #response = await session.request(method='GET', url=url)\n",
    "    response = await session.get(url=url)\n",
    "\n",
    "    return response\n",
    "\n",
    "for i in range(1,5):\n",
    "    start = time.time() # start time for timing event\n",
    "    #async with aiohttp.ClientSession() as session: #use aiohttp\n",
    "    session = httpx.AsyncClient() #use httpx\n",
    "    await asyncio.gather(*[call_url(session) for x in range(i)])\n",
    "    await session.aclose()\n",
    "    print(f'{i} call(s) in {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    start = time.time()\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "    #async with httpx.AsyncClient as session:\n",
    "        await asyncio.gather(*[call_url(session) for x in range(i)])\n",
    "    print(f'{i} call(s) in {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import time\n",
    "import httpx\n",
    "\n",
    "async def call_url(session):\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollections\"\n",
    "    #async with aiohttp.ClientSession() as session: #use aiohttp\n",
    "    async with httpx.AsyncClient as session:  #use httpx\n",
    "        response = await session.get(url=url)\n",
    "\n",
    "    return response\n",
    "\n",
    "for i in range(1,5):\n",
    "    start = time.time() # start time for timing event\n",
    "    await asyncio.gather(*[call_url(session) for x in range(i)])\n",
    "    print(f'{i} call(s) in {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(series_list))\n",
    "print(series_list[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_df = pd.DataFrame.from_dict(series_list)\n",
    "study_df.to_csv(f'data/{tcia_collections[0][\"Collection\"]}_studies_series.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(study_df)\n",
    "with open('data/TCGA-GBM-series.json',\"w\") as f:\n",
    "    f.write(str(series_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_sample = series[0]    \n",
    "study_sample = studies[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "series_fields = [x for x in series_sample]\n",
    "study_fields = [x for x in study_sample]\n",
    "print(len(series_fields))\n",
    "print(len(study_fields))\n",
    "merged = list(set(series_fields).union(set(study_fields)))\n",
    "\n",
    "merged2 = {**study_sample, **series_sample}\n",
    "\n",
    "print(study_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sample.update(series_sample)\n",
    "print(len(study_sample))\n",
    "print(study_sample)\n",
    "\n",
    "merged2 = {**study_sample, **series_sample}\n",
    "print(len(merged2))\n",
    "print(merged2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sample.update(series_sample)\n",
    "print(len(study_sample))\n",
    "study_sample\n",
    "from collections import OrderedDict\n",
    "od = OrderedDict(study_sample)\n",
    "od\n",
    "\n",
    "x = []\n",
    "x.append(study_sample)\n",
    "\n",
    "df = pd.DataFrame.from_dict(x)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(series_sample)\n",
    "print(study_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSeries?Collection=TCGA-GBM&StudyInstanceUID=1.3.6.1.4.1.14519.5.2.1.7695.4001.130563880911723253267280582465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studies = [s['StudyInstanceUID'] for s in res.json()]\n",
    "#for x in res.json():\n",
    "#    print(x['StudyInstanceUID'])\n",
    "studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for x in collections.json()[:2]:\n",
    "        print(x['Collection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with all the studies\n",
    "studies = [s['StudyInstanceUID'] for s in res.json()]\n",
    "\n",
    "\n",
    "#storageConnString = os.environ[\"AzureWebJobsStorage\"]\n",
    "storageConnString = 'DefaultEndpointsProtocol=https;AccountName=sjbfunctest;AccountKey=XuYBliYrXazCmfDdK2jLcaJcfqPgu8tC43TlltTMY413nusjx2N6+IvErYmVXuZfOBVgVaCQ52RObKioS9FDRg==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "#x = asq.QueueService(account_name='sjbfunctest', account_key='mykey')\n",
    "#service = asq.QueueServiceClient.from_connection_string(conn_str=connection_string)\n",
    "patient_studies_queue = asq.QueueClient.from_connection_string(conn_str=storageConnString,queue_name='studies')\n",
    "\n",
    "# Create the queue if it doesn't exist...  by exception\n",
    "#   Which is hacky, but effective\n",
    "try:\n",
    "    patient_studies_queue.get_queue_properties()\n",
    "except:\n",
    "    patient_studies_queue.create_queue()\n",
    "\n",
    "# Must base-64 encode since... functions...\n",
    "enc = asq.TextBase64EncodePolicy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for study in studies:\n",
    "    b64 = enc.encode(study)\n",
    "    patient_studies_queue.send_message(b64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(studies)\n",
    "study_id = studies[0]\n",
    "study_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_id = studies[0]\n",
    "\n",
    "urlGetSeries = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSeries'\n",
    "params = {'StudyInstanceUID': study_id}\n",
    "res = requests.get(urlGetSeries,params=params,timeout=None) #timeout=15.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.json()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with all the studies\n",
    "series = [s['SeriesInstanceUID'] for s in res.json()]\n",
    "\n",
    "series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storageConnString = os.environ[\"AzureWebJobsStorage\"]\n",
    "storageConnString = 'DefaultEndpointsProtocol=https;AccountName=sjbfunctest;AccountKey=XuYBliYrXazCmfDdK2jLcaJcfqPgu8tC43TlltTMY413nusjx2N6+IvErYmVXuZfOBVgVaCQ52RObKioS9FDRg==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "series_queue = asq.QueueClient.from_connection_string(conn_str=storageConnString,queue_name='series')\n",
    "\n",
    "# Create the queue if it doesn't exist...  by exception\n",
    "#   Which is hacky, but effective\n",
    "try:\n",
    "    series_queue.get_queue_properties()\n",
    "except:\n",
    "    series_queue.create_queue()\n",
    "\n",
    "# Must base-64 encode since... functions...\n",
    "enc = asq.TextBase64EncodePolicy()\n",
    "\n",
    "for s in series[:1]:\n",
    "    b64 = enc.encode(s)\n",
    "    series_queue.send_message(b64)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the zip files\n",
    "url = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getImage?SeriesInstanceUID=1.3.6.1.4.1.14519.5.2.1.7695.4001.306204232344341694648035234440'\n",
    "res = requests.get(url,timeout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Get the study id from the base-64 encoded incoming queue\n",
    "#series_id = msg.get_body().decode('utf-8')\n",
    "series_id = series[0]\n",
    "\n",
    "series_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlGetImage = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getImage'\n",
    "params = {'SeriesInstanceUID': series_id}\n",
    "res = requests.get(urlGetImage,params=params,timeout=None) #timeout=15.0)\n",
    "print(res.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "\n",
    "def download_file(url):\n",
    "    local_filename = \"foo4.zip\"\n",
    "    with requests.get(urlGetImage, stream=True) as r:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_filename = \"food4.zip\"\n",
    "with requests.get(urlGetImage,params=params,timeout=None, stream=True) as r:\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        shutil.copyfileobj(r.raw, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "file_like_object = io.BytesIO(res.content)\n",
    "z = zipfile.ZipFile(file_like_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = z.filelist\n",
    "f1 = files[1]\n",
    "\n",
    "#for f in files:\n",
    "    #print(f)\n",
    "    #z.read(f)\n",
    "dcmbytes = z.read(f)\n",
    "#dcmbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install azure.storage.blob\n",
    "\n",
    "import azure.storage.blob as blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = f1.filename.split('/')\n",
    "dcm_names = [p for p in parts if p.find('.dcm') != -1]\n",
    "if len(dcm_names) > 0:\n",
    "    dcm_name = dcm_names[0]\n",
    "dcm_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageConnString = 'DefaultEndpointsProtocol=https;AccountName=sjbfunctest;AccountKey=XuYBliYrXazCmfDdK2jLcaJcfqPgu8tC43TlltTMY413nusjx2N6+IvErYmVXuZfOBVgVaCQ52RObKioS9FDRg==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "b = blob.ContainerClient.from_connection_string(conn_str=storageConnString,container_name='dicoms2')\n",
    "\n",
    "# Create the queue if it doesn't exist...  by exception\n",
    "#   Which is hacky, but effective\n",
    "try:\n",
    "    b.get_container_properties()\n",
    "except:\n",
    "    b.create_container()\n",
    "\n",
    "for f in files:\n",
    "    dicom_file = z.read(f)\n",
    "    parts = f.filename.split('/')\n",
    "    dcm_parts = [p for p in parts if p.find('.dcm') != -1]\n",
    "    if len(dcm_parts) == 1: # we have a dicom file, and only one\n",
    "        dcm_name = f'{series_id}/{dcm_parts[0]}'\n",
    "        print(dcm_name)\n",
    "        up = b.upload_blob(data=z.read(f), name=dcm_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#upblob = b.upload_blob(data=dcmbytes,name='test3.dcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upblob.blob_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "storageConnString = 'DefaultEndpointsProtocol=https;AccountName=sjbfunctest;AccountKey=XuYBliYrXazCmfDdK2jLcaJcfqPgu8tC43TlltTMY413nusjx2N6+IvErYmVXuZfOBVgVaCQ52RObKioS9FDRg==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "series_queue = asq.QueueClient.from_connection_string(conn_str=storageConnString,queue_name='series')\n",
    "\n",
    "# Create the queue if it doesn't exist...  by exception\n",
    "#   Which is hacky, but effective\n",
    "try:\n",
    "    series_queue.get_queue_properties()\n",
    "except:\n",
    "    series_queue.create_queue()\n",
    "\n",
    "# Must base-64 encode since... functions...\n",
    "enc = asq.TextBase64EncodePolicy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create a list with all the studies\n",
    "    series = [s['SeriesInstanceUID'] for s in res.json()]\n",
    "\n",
    "\n",
    "    storageConnString = os.environ[\"AzureWebJobsStorage\"]\n",
    "  \n",
    "    series_queue = asq.QueueClient.from_connection_string(conn_str=storageConnString,queue_name='series')\n",
    "\n",
    "    # Create the queue if it doesn't exist...  by exception\n",
    "    #   Which is hacky, but effective\n",
    "    try:\n",
    "        series_queue.get_queue_properties()\n",
    "    except:\n",
    "        series_queue.create_queue()\n",
    "\n",
    "    # Must base-64 encode since... functions...\n",
    "    enc = asq.TextBase64EncodePolicy()\n",
    "\n",
    "    for s in series:\n",
    "        b64 = enc.encode(s)\n",
    "        series_queue.send_message(b64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "import os\n",
    "\n",
    "async def download(url:str, folder:str):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    resp = await httpx.get(url)\n",
    "    resp.raise_for_status()\n",
    "    async with aiofiles.open(os.path.join(folder, filename), \"wb\") as f:\n",
    "        await f.write(resp.content)\n",
    "\n",
    "\n",
    "async def download_all_photos(out_dir: str):\n",
    "    resp = await httpx.get(\"https://jsonplaceholder.typicode.com/photos\")\n",
    "    resp.raise_for_status()\n",
    "    urls = list(set(d[\"url\"] for d in resp.json()))[:100]\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    await asyncio.gather(*[download(url, out_dir) for url in urls])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(download_all_photos('100_photos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WORKS\n",
    "\n",
    "import httpx\n",
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "import os\n",
    "\n",
    "async def download(url:str, folder:str):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    async with httpx.AsyncClient() as session:\n",
    "        resp = await session.get(url)\n",
    "        resp.raise_for_status()\n",
    "    async with aiofiles.open(os.path.join(folder, filename), \"wb\") as f:\n",
    "        await f.write(resp.content)\n",
    "        \n",
    "async def download_all_photos(out_dir: str):\n",
    "    async with httpx.AsyncClient() as session:\n",
    "        resp = await session.get(\"https://jsonplaceholder.typicode.com/photos\")\n",
    "        resp.raise_for_status()\n",
    "    urls = list(set(d[\"url\"] for d in resp.json()))[:100]\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    await asyncio.gather(*[download(url, out_dir) for url in urls])\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    asyncio.run(download_all_photos('100_photos'))\n",
    "    \n",
    "await download_all_photos('100_photos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stuff(f):\n",
    "    f.write('And stuff with context passed to another method. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stuff(f):\n",
    "    f.write('And stuff with context passed to another method. ')\n",
    "    \n",
    "with open('foo.txt',\"w\") as f:\n",
    "    f.write('Start with context manager inside with statement. ')\n",
    "    write_stuff(f)\n",
    "    f.write('And back to close the with.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show httpx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! What is your name?\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello! What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-4d0f01fe418b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'name' is not defined"
     ]
    }
   ],
   "source": [
    "input = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, \" name \"! How are you doing, \" name \"? My name is Bodie, \" name \".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
