{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! What is your name?\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello! What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bill nye the science pie\n"
     ]
    }
   ],
   "source": [
    "name = input().upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, BILL NYE THE SCIENCE PIE! How are you doing, BILL NYE THE SCIENCE PIE? My name is Bodie, BILL NYE THE SCIENCE PIE.\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, \" +  name + \"! How are you doing, \" + name + \"? My name is Bodie, \" + name + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-25089fca76b1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-32-25089fca76b1>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    <button>that aint me name</button>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-bc3759a6576d>\u001b[0m in \u001b[0;36masync-def-wrapper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{i} runs in {time.time() - start} seconds'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "##  This works with HTTPX\n",
    "async def foo():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        url = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollectionValues'\n",
    "        r = await client.get(url) #'https://www.example.com/')\n",
    "    return r\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    await asyncio.gather(*[foo() for x in range(i)])\n",
    "    print(f'{i} runs in {time.time() - start} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call TCIA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import azure.storage.queue as asq\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# variables used throughout\n",
    "tciabase = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleRateLimits(headers):\n",
    "    limits = dict([(h,int(headers[h])) for h in headers if h.find('x-rate') != -1])\n",
    "    # Example : {'x-ratelimit-limit-hour': 360000, 'x-ratelimit-remaining-hour': 359961, 'x-ratelimit-limit-second': 1000, 'x-ratelimit-remaining-second': 999}\n",
    "    \n",
    "    # TODO: send a queue message to a queue which will 'pause' the querying from TCIA for the right time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getCollectionsAsync():\n",
    "    urlGetCollections = f'{tciabase}/getCollectionValues'\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        r = await client.get(urlGetCollections)\n",
    "    \n",
    "    # not really needed here, but should send every time we send a request\n",
    "    handleRateLimits(r.headers)\n",
    "    \n",
    "    if r is not None:\n",
    "        #return [c['Collection'] for c in r.json()]\n",
    "        return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tcia_collections = await getCollectionsAsync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Collection': 'TCGA-GBM'}, {'Collection': 'LIDC-IDRI'}, {'Collection': 'BREAST-DIAGNOSIS'}, {'Collection': 'PROSTATE-MRI'}, {'Collection': 'PROSTATE-DIAGNOSIS'}, {'Collection': 'NaF PROSTATE'}, {'Collection': 'CT COLONOGRAPHY'}, {'Collection': 'REMBRANDT'}, {'Collection': 'RIDER Breast MRI'}, {'Collection': 'RIDER Lung CT'}, {'Collection': 'RIDER NEURO MRI'}, {'Collection': 'RIDER PHANTOM MRI'}, {'Collection': 'RIDER PHANTOM PET-CT'}, {'Collection': 'RIDER Lung PET-CT'}, {'Collection': 'QIBA CT-1C'}, {'Collection': 'Phantom FDA'}, {'Collection': 'TCGA-BRCA'}, {'Collection': 'QIN-BREAST'}, {'Collection': 'TCGA-LGG'}, {'Collection': 'TCGA-KIRC'}, {'Collection': 'TCGA-LUAD'}, {'Collection': 'Prostate-3T'}, {'Collection': 'QIN-HEADNECK'}, {'Collection': 'TCGA-PRAD'}, {'Collection': 'NSCLC Radiogenomics'}, {'Collection': 'Head-Neck Cetuximab'}, {'Collection': 'TCGA-LIHC'}, {'Collection': 'TCGA-KIRP'}, {'Collection': 'TCGA-OV'}, {'Collection': 'TCGA-HNSC'}, {'Collection': 'TCGA-KICH'}, {'Collection': 'TCGA-BLCA'}, {'Collection': 'TCGA-LUSC'}, {'Collection': 'TCGA-COAD'}, {'Collection': 'TCGA-THCA'}, {'Collection': 'TCGA-READ'}, {'Collection': 'NSCLC-Radiomics-Genomics'}, {'Collection': 'Lung Phantom'}, {'Collection': 'NSCLC-Radiomics'}, {'Collection': 'QIN Breast DCE-MRI'}, {'Collection': 'QIN PET Phantom'}, {'Collection': 'ISPY1'}, {'Collection': 'TCGA-UCEC'}, {'Collection': 'QIN GBM Treatment Response'}, {'Collection': 'SPIE-AAPM Lung CT Challenge'}, {'Collection': 'TCGA-ESCA'}, {'Collection': 'TCGA-STAD'}, {'Collection': 'TCGA-CESC'}, {'Collection': 'QIN LUNG CT'}, {'Collection': 'TCGA-SARC'}, {'Collection': 'LungCT-Diagnosis'}, {'Collection': 'CT Lymph Nodes'}, {'Collection': 'Mouse-Astrocytoma'}, {'Collection': 'Mouse-Mammary'}, {'Collection': 'Prostate Fused-MRI-Pathology'}, {'Collection': 'Soft-tissue-Sarcoma'}, {'Collection': '4D-Lung'}, {'Collection': 'Breast-MRI-NACT-Pilot'}, {'Collection': 'Pancreas-CT'}, {'Collection': 'CBIS-DDSM'}, {'Collection': 'PROSTATEx'}, {'Collection': 'HNSCC'}, {'Collection': 'IvyGAP'}, {'Collection': 'Head-Neck-PET-CT'}, {'Collection': 'LCTSC'}, {'Collection': 'LGG-1p19qDeletion'}, {'Collection': 'CC-Radiomics-Phantom'}, {'Collection': 'ACRIN-FLT-Breast'}, {'Collection': 'CPTAC-PDA'}, {'Collection': 'CPTAC-UCEC'}, {'Collection': 'CPTAC-CCRCC'}, {'Collection': 'CPTAC-LUAD'}, {'Collection': 'CPTAC-GBM'}, {'Collection': 'CPTAC-LSCC'}, {'Collection': 'CPTAC-CM'}, {'Collection': 'Brain-Tumor-Progression'}, {'Collection': 'QIN-PROSTATE-Repeatability'}, {'Collection': 'APOLLO'}, {'Collection': 'CPTAC-HNSCC'}, {'Collection': 'Anti-PD-1_MELANOMA'}, {'Collection': 'MRI-DIR'}, {'Collection': 'Lung-Fused-CT-Pathology'}, {'Collection': 'HNSCC-3DCT-RT'}, {'Collection': 'ACRIN-FMISO-Brain'}, {'Collection': 'Anti-PD-1_Lung'}, {'Collection': 'ACRIN-NSCLC-FDG-PET'}, {'Collection': 'LDCT-and-Projection-data'}, {'Collection': 'ACRIN-DSC-MR-Brain'}, {'Collection': 'CC-Radiomics-Phantom-2'}, {'Collection': 'VICTRE'}, {'Collection': 'CC-Radiomics-Phantom-3'}, {'Collection': 'NSCLC-Radiomics-Interobserver1'}, {'Collection': 'CPTAC-SAR'}, {'Collection': 'AAPM-RT-MAC'}, {'Collection': 'C4KC-KiTS'}, {'Collection': 'Lung-PET-CT-Dx'}, {'Collection': 'OPC-Radiomics'}, {'Collection': 'QIN-BRAIN-DSC-MRI'}, {'Collection': 'PDMR-BL0293-F563'}, {'Collection': 'Pelvic-Reference-Data'}, {'Collection': 'HEAD-NECK-RADIOMICS-HN1'}, {'Collection': 'PDMR-292921-168-R'}, {'Collection': 'Prostate-MRI-US-Biopsy'}, {'Collection': 'DRO-Toolkit'}, {'Collection': 'COVID-19-AR'}]\n"
     ]
    }
   ],
   "source": [
    "#len(tcia_collections)\n",
    "#print(tcia_collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://services.cancerimagingarchive.net/services/v3/TCIA/query/getPatientStudy?Collection=TCGA-GBM\n",
    "# https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSeries?Collection=TCGA-GBM&StudyInstanceUID=1.3.6.1.4.1.14519.5.2.1.7695.4001.130563880911723253267280582465  \n",
    "# https://services.cancerimagingarchive.net/services/v3/TCIA/query/getImage?SeriesInstanceUID=1.3.6.1.4.1.14519.5.2.1.7695.4001.306204232344341694648035234440\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get PatientStudies in Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-68a7df453531>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-68a7df453531>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ------------------------------------------------------------------------------------------------------\u001b[0m\n\u001b[1;37m                                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#studies = await getPatientStudiesPerCollection(collections[0]['Collection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(studies))\n",
    "#print(studies[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#study_ids = [s['StudyInstanceUID'] for s in studies] # if s.find('StudyInstanceUID') != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getSeriesPerStudy(study, async_client):\n",
    "    urlGetSeriesBase = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSeries'\n",
    "    params = {'StudyInstanceUID': study}\n",
    "    #async with httpx.AsyncClient() as client:\n",
    "    r = await async_client.get(urlGetSeriesBase,params=params,timeout=None) #timeout=15.0)\n",
    "    \n",
    "    # not really needed here, but should send every time we send a request\n",
    "    handleRateLimits(r.headers)\n",
    "    \n",
    "    if r is not None:\n",
    "        #return [c['Collection'] for c in r.json()]\n",
    "        return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#series = await getSeriesPerStudy(studies[0]['StudyInstanceUID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(series)\n",
    "#print(series[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getSeriesPerStudy(study, async_client):\n",
    "    urlGetSeriesBase = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSeries'\n",
    "    params = {'StudyInstanceUID': study}\n",
    "    #async with httpx.AsyncClient() as client:\n",
    "    r = await async_client.get(urlGetSeriesBase,params=params,timeout=None) #timeout=15.0)\n",
    "    \n",
    "    # not really needed here, but should send every time we send a request\n",
    "    handleRateLimits(r.headers)\n",
    "    \n",
    "    if r is not None:\n",
    "        #return [c['Collection'] for c in r.json()]\n",
    "        return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getSeriesPerStudyJoinDict(study_id, study_dict, async_client):\n",
    "    \n",
    "    #retry three times if necessary\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            series = await getSeriesPerStudy(study_id, async_client)\n",
    "            # join with the study_dict we already have\n",
    "                    #create a list of series dicts (combining metadata from study)\n",
    "            # merge the dictionaries using ** to unpack the dictionaries (since .union is in place)\n",
    "            series_list = [] # list to store all the series for a collection\n",
    "\n",
    "            for s in series:\n",
    "                merged_dict = {**study_dict, **s}\n",
    "                series_list.append(merged_dict)        \n",
    "            return series_list\n",
    "        except:\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "    \n",
    "    # if we get here we have a problem that's happened three times\n",
    "    raise Exception(\"Failed to get series over set of retries\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getInstancesPerSeries(series, async_client):\n",
    "    urlGetInstancesBase = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSOPInstanceUIDs'\n",
    "    params = {'SeriesInstanceUID': series}\n",
    "    #async with httpx.AsyncClient() as client:\n",
    "    r = await async_client.get(urlGetInstancesBase,params=params,timeout=None) #timeout=15.0)\n",
    "    \n",
    "    # not really needed here, but should send every time we send a request\n",
    "    handleRateLimits(r.headers)\n",
    "    \n",
    "    if r is not None:\n",
    "        #return [c['Collection'] for c in r.json()]\n",
    "        return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getInstancesPerSeriesJoinDict(series_id, series_dict, async_client):\n",
    "    \n",
    "    #retry, if necessary\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            instances = await getInstancesPerSeries(series_id, async_client)\n",
    "            # join with the study_dict we already have\n",
    "                    #create a list of series dicts (combining metadata from study)\n",
    "            # merge the dictionaries using ** to unpack the dictionaries (since .union is in place)\n",
    "            instances_list = [] # list to store all the series for a collection\n",
    "\n",
    "            for s in instances:\n",
    "                merged_dict = {**series_dict, **s}\n",
    "                instances_list.append(merged_dict)   \n",
    "            return instances_list\n",
    "        except:\n",
    "            time.sleep(i)\n",
    "            continue\n",
    "    \n",
    "    # if we get here we have a problem that's happened three times\n",
    "    raise Exception(\"Failed to get series over set of retries\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the collections. We'll operate on one at a time\n",
    "\n",
    "tcia_collections = await getCollectionsAsync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Collection': 'TCGA-GBM'},\n",
       " {'Collection': 'LIDC-IDRI'},\n",
       " {'Collection': 'BREAST-DIAGNOSIS'},\n",
       " {'Collection': 'PROSTATE-MRI'},\n",
       " {'Collection': 'PROSTATE-DIAGNOSIS'},\n",
       " {'Collection': 'NaF PROSTATE'},\n",
       " {'Collection': 'CT COLONOGRAPHY'},\n",
       " {'Collection': 'REMBRANDT'},\n",
       " {'Collection': 'RIDER Breast MRI'},\n",
       " {'Collection': 'RIDER Lung CT'},\n",
       " {'Collection': 'RIDER NEURO MRI'},\n",
       " {'Collection': 'RIDER PHANTOM MRI'},\n",
       " {'Collection': 'RIDER PHANTOM PET-CT'},\n",
       " {'Collection': 'RIDER Lung PET-CT'},\n",
       " {'Collection': 'QIBA CT-1C'},\n",
       " {'Collection': 'Phantom FDA'},\n",
       " {'Collection': 'TCGA-BRCA'},\n",
       " {'Collection': 'QIN-BREAST'},\n",
       " {'Collection': 'TCGA-LGG'},\n",
       " {'Collection': 'TCGA-KIRC'},\n",
       " {'Collection': 'TCGA-LUAD'},\n",
       " {'Collection': 'Prostate-3T'},\n",
       " {'Collection': 'QIN-HEADNECK'},\n",
       " {'Collection': 'TCGA-PRAD'},\n",
       " {'Collection': 'NSCLC Radiogenomics'},\n",
       " {'Collection': 'Head-Neck Cetuximab'},\n",
       " {'Collection': 'TCGA-LIHC'},\n",
       " {'Collection': 'TCGA-KIRP'},\n",
       " {'Collection': 'TCGA-OV'},\n",
       " {'Collection': 'TCGA-HNSC'},\n",
       " {'Collection': 'TCGA-KICH'},\n",
       " {'Collection': 'TCGA-BLCA'},\n",
       " {'Collection': 'TCGA-LUSC'},\n",
       " {'Collection': 'TCGA-COAD'},\n",
       " {'Collection': 'TCGA-THCA'},\n",
       " {'Collection': 'TCGA-READ'},\n",
       " {'Collection': 'NSCLC-Radiomics-Genomics'},\n",
       " {'Collection': 'Lung Phantom'},\n",
       " {'Collection': 'NSCLC-Radiomics'},\n",
       " {'Collection': 'QIN Breast DCE-MRI'},\n",
       " {'Collection': 'QIN PET Phantom'},\n",
       " {'Collection': 'ISPY1'},\n",
       " {'Collection': 'TCGA-UCEC'},\n",
       " {'Collection': 'QIN GBM Treatment Response'},\n",
       " {'Collection': 'SPIE-AAPM Lung CT Challenge'},\n",
       " {'Collection': 'TCGA-ESCA'},\n",
       " {'Collection': 'TCGA-STAD'},\n",
       " {'Collection': 'TCGA-CESC'},\n",
       " {'Collection': 'QIN LUNG CT'},\n",
       " {'Collection': 'TCGA-SARC'},\n",
       " {'Collection': 'LungCT-Diagnosis'},\n",
       " {'Collection': 'CT Lymph Nodes'},\n",
       " {'Collection': 'Mouse-Astrocytoma'},\n",
       " {'Collection': 'Mouse-Mammary'},\n",
       " {'Collection': 'Prostate Fused-MRI-Pathology'},\n",
       " {'Collection': 'Soft-tissue-Sarcoma'},\n",
       " {'Collection': '4D-Lung'},\n",
       " {'Collection': 'Breast-MRI-NACT-Pilot'},\n",
       " {'Collection': 'Pancreas-CT'},\n",
       " {'Collection': 'CBIS-DDSM'},\n",
       " {'Collection': 'PROSTATEx'},\n",
       " {'Collection': 'HNSCC'},\n",
       " {'Collection': 'IvyGAP'},\n",
       " {'Collection': 'Head-Neck-PET-CT'},\n",
       " {'Collection': 'LCTSC'},\n",
       " {'Collection': 'LGG-1p19qDeletion'},\n",
       " {'Collection': 'CC-Radiomics-Phantom'},\n",
       " {'Collection': 'ACRIN-FLT-Breast'},\n",
       " {'Collection': 'CPTAC-PDA'},\n",
       " {'Collection': 'CPTAC-UCEC'},\n",
       " {'Collection': 'CPTAC-CCRCC'},\n",
       " {'Collection': 'CPTAC-LUAD'},\n",
       " {'Collection': 'CPTAC-GBM'},\n",
       " {'Collection': 'CPTAC-LSCC'},\n",
       " {'Collection': 'CPTAC-CM'},\n",
       " {'Collection': 'Brain-Tumor-Progression'},\n",
       " {'Collection': 'APOLLO'},\n",
       " {'Collection': 'CPTAC-HNSCC'},\n",
       " {'Collection': 'Anti-PD-1_MELANOMA'},\n",
       " {'Collection': 'MRI-DIR'},\n",
       " {'Collection': 'Lung-Fused-CT-Pathology'},\n",
       " {'Collection': 'HNSCC-3DCT-RT'},\n",
       " {'Collection': 'ACRIN-FMISO-Brain'},\n",
       " {'Collection': 'Anti-PD-1_Lung'},\n",
       " {'Collection': 'ACRIN-NSCLC-FDG-PET'},\n",
       " {'Collection': 'LDCT-and-Projection-data'},\n",
       " {'Collection': 'CC-Radiomics-Phantom-2'},\n",
       " {'Collection': 'VICTRE'},\n",
       " {'Collection': 'CC-Radiomics-Phantom-3'},\n",
       " {'Collection': 'NSCLC-RADIOMICS-INTEROBSERVER1'},\n",
       " {'Collection': 'CPTAC-SAR'},\n",
       " {'Collection': 'AAPM-RT-MAC'},\n",
       " {'Collection': 'C4KC-KiTS'},\n",
       " {'Collection': 'OPC-Radiomics'},\n",
       " {'Collection': 'QIN-BRAIN-DSC-MRI'},\n",
       " {'Collection': 'PDMR-BL0293-F563'},\n",
       " {'Collection': 'Pelvic-Reference-Data'},\n",
       " {'Collection': 'HEAD-NECK-RADIOMICS-HN1'},\n",
       " {'Collection': 'GBM-DSC-MRI-DRO'},\n",
       " {'Collection': 'DRO-Toolkit'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcia_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already processed: 0\n",
      "getting studies\n",
      "studies in TCGA-GBM : 575\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Studies 0-10 in 33.315736055374146 seconds\n",
      "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Studies 10-20 in 31.111398935317993 seconds\n",
      "[20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "Studies 20-30 in 31.08149242401123 seconds\n",
      "[30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "Studies 30-40 in 30.402886390686035 seconds\n",
      "[40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "Studies 40-50 in 31.92508578300476 seconds\n",
      "[50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
      "Studies 50-60 in 29.77124834060669 seconds\n",
      "[60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "Studies 60-70 in 30.29601740837097 seconds\n",
      "[70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "Studies 70-80 in 30.1108078956604 seconds\n",
      "[80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
      "Studies 80-90 in 33.08348560333252 seconds\n",
      "[90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "Studies 90-100 in 36.555383920669556 seconds\n",
      "[100, 101, 102, 103, 104, 105, 106, 107, 108, 109]\n",
      "Studies 100-110 in 35.85938000679016 seconds\n",
      "[110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n",
      "Studies 110-120 in 34.309550523757935 seconds\n",
      "[120, 121, 122, 123, 124, 125, 126, 127, 128, 129]\n",
      "Studies 120-130 in 34.9603636264801 seconds\n",
      "[130, 131, 132, 133, 134, 135, 136, 137, 138, 139]\n",
      "Studies 130-140 in 35.07364058494568 seconds\n",
      "[140, 141, 142, 143, 144, 145, 146, 147, 148, 149]\n",
      "Studies 140-150 in 35.15775465965271 seconds\n",
      "[150, 151, 152, 153, 154, 155, 156, 157, 158, 159]\n",
      "Studies 150-160 in 30.20036792755127 seconds\n",
      "[160, 161, 162, 163, 164, 165, 166, 167, 168, 169]\n",
      "Studies 160-170 in 30.139409065246582 seconds\n",
      "[170, 171, 172, 173, 174, 175, 176, 177, 178, 179]\n",
      "Studies 170-180 in 30.579552173614502 seconds\n",
      "[180, 181, 182, 183, 184, 185, 186, 187, 188, 189]\n",
      "Studies 180-190 in 31.120982885360718 seconds\n",
      "[190, 191, 192, 193, 194, 195, 196, 197, 198, 199]\n",
      "Studies 190-200 in 31.37551474571228 seconds\n",
      "[200, 201, 202, 203, 204, 205, 206, 207, 208, 209]\n",
      "Studies 200-210 in 31.71539330482483 seconds\n",
      "[210, 211, 212, 213, 214, 215, 216, 217, 218, 219]\n",
      "Studies 210-220 in 30.46144723892212 seconds\n",
      "[220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
      "Studies 220-230 in 30.5428569316864 seconds\n",
      "[230, 231, 232, 233, 234, 235, 236, 237, 238, 239]\n",
      "Studies 230-240 in 32.56895399093628 seconds\n",
      "[240, 241, 242, 243, 244, 245, 246, 247, 248, 249]\n",
      "Studies 240-250 in 30.837735652923584 seconds\n",
      "[250, 251, 252, 253, 254, 255, 256, 257, 258, 259]\n",
      "Studies 250-260 in 30.56930708885193 seconds\n",
      "[260, 261, 262, 263, 264, 265, 266, 267, 268, 269]\n",
      "Studies 260-270 in 31.239254236221313 seconds\n",
      "[270, 271, 272, 273, 274, 275, 276, 277, 278, 279]\n",
      "Studies 270-280 in 32.33166480064392 seconds\n",
      "[280, 281, 282, 283, 284, 285, 286, 287, 288, 289]\n",
      "Studies 280-290 in 31.581973791122437 seconds\n",
      "[290, 291, 292, 293, 294, 295, 296, 297, 298, 299]\n",
      "Studies 290-300 in 30.743871688842773 seconds\n",
      "[300, 301, 302, 303, 304, 305, 306, 307, 308, 309]\n",
      "Studies 300-310 in 31.279088497161865 seconds\n",
      "[310, 311, 312, 313, 314, 315, 316, 317, 318, 319]\n",
      "Studies 310-320 in 30.490059852600098 seconds\n",
      "[320, 321, 322, 323, 324, 325, 326, 327, 328, 329]\n",
      "Studies 320-330 in 30.717174768447876 seconds\n",
      "[330, 331, 332, 333, 334, 335, 336, 337, 338, 339]\n",
      "Studies 330-340 in 33.333287477493286 seconds\n",
      "[340, 341, 342, 343, 344, 345, 346, 347, 348, 349]\n",
      "Studies 340-350 in 31.353782653808594 seconds\n",
      "[350, 351, 352, 353, 354, 355, 356, 357, 358, 359]\n",
      "Studies 350-360 in 30.014617443084717 seconds\n",
      "[360, 361, 362, 363, 364, 365, 366, 367, 368, 369]\n",
      "Studies 360-370 in 30.89264965057373 seconds\n",
      "[370, 371, 372, 373, 374, 375, 376, 377, 378, 379]\n",
      "Studies 370-380 in 31.50693106651306 seconds\n",
      "[380, 381, 382, 383, 384, 385, 386, 387, 388, 389]\n",
      "Studies 380-390 in 30.864609479904175 seconds\n",
      "[390, 391, 392, 393, 394, 395, 396, 397, 398, 399]\n",
      "Studies 390-400 in 31.44306230545044 seconds\n",
      "[400, 401, 402, 403, 404, 405, 406, 407, 408, 409]\n",
      "Studies 400-410 in 32.73991775512695 seconds\n",
      "[410, 411, 412, 413, 414, 415, 416, 417, 418, 419]\n",
      "Studies 410-420 in 34.75353956222534 seconds\n",
      "[420, 421, 422, 423, 424, 425, 426, 427, 428, 429]\n",
      "Studies 420-430 in 35.79590320587158 seconds\n",
      "[430, 431, 432, 433, 434, 435, 436, 437, 438, 439]\n",
      "Studies 430-440 in 34.69409465789795 seconds\n",
      "[440, 441, 442, 443, 444, 445, 446, 447, 448, 449]\n",
      "Studies 440-450 in 34.1099112033844 seconds\n",
      "[450, 451, 452, 453, 454, 455, 456, 457, 458, 459]\n",
      "Studies 450-460 in 34.10898780822754 seconds\n",
      "[460, 461, 462, 463, 464, 465, 466, 467, 468, 469]\n",
      "Studies 460-470 in 32.24931621551514 seconds\n",
      "[470, 471, 472, 473, 474, 475, 476, 477, 478, 479]\n",
      "Studies 470-480 in 32.99730205535889 seconds\n",
      "[480, 481, 482, 483, 484, 485, 486, 487, 488, 489]\n",
      "Studies 480-490 in 31.22222328186035 seconds\n",
      "[490, 491, 492, 493, 494, 495, 496, 497, 498, 499]\n",
      "Studies 490-500 in 31.111393213272095 seconds\n",
      "[500, 501, 502, 503, 504, 505, 506, 507, 508, 509]\n",
      "Studies 500-510 in 32.78677320480347 seconds\n",
      "[510, 511, 512, 513, 514, 515, 516, 517, 518, 519]\n",
      "Studies 510-520 in 33.3019015789032 seconds\n",
      "[520, 521, 522, 523, 524, 525, 526, 527, 528, 529]\n",
      "Studies 520-530 in 39.75956869125366 seconds\n",
      "[530, 531, 532, 533, 534, 535, 536, 537, 538, 539]\n",
      "Studies 530-540 in 34.439260959625244 seconds\n",
      "[540, 541, 542, 543, 544, 545, 546, 547, 548, 549]\n",
      "Studies 540-550 in 34.50273132324219 seconds\n",
      "[550, 551, 552, 553, 554, 555, 556, 557, 558, 559]\n",
      "Studies 550-560 in 35.09873580932617 seconds\n",
      "[560, 561, 562, 563, 564, 565, 566, 567, 568, 569]\n",
      "Studies 560-570 in 36.933976888656616 seconds\n",
      "[570, 571, 572, 573, 574]\n",
      "Studies 570-580 in 20.59676504135132 seconds\n",
      "Already processed LIDC-IDRI. Please check the data folder.\n",
      "Already processed BREAST-DIAGNOSIS. Please check the data folder.\n",
      "Already processed PROSTATE-MRI. Please check the data folder.\n",
      "Already processed PROSTATE-DIAGNOSIS. Please check the data folder.\n",
      "Already processed NaF PROSTATE. Please check the data folder.\n",
      "Already processed CT COLONOGRAPHY. Please check the data folder.\n",
      "Already processed REMBRANDT. Please check the data folder.\n",
      "Already processed RIDER Breast MRI. Please check the data folder.\n",
      "Already processed RIDER Lung CT. Please check the data folder.\n",
      "Already processed RIDER NEURO MRI. Please check the data folder.\n",
      "Already processed RIDER PHANTOM MRI. Please check the data folder.\n",
      "Already processed RIDER PHANTOM PET-CT. Please check the data folder.\n",
      "Already processed RIDER Lung PET-CT. Please check the data folder.\n",
      "Already processed QIBA CT-1C. Please check the data folder.\n",
      "Already processed Phantom FDA. Please check the data folder.\n",
      "Already processed TCGA-BRCA. Please check the data folder.\n",
      "Already processed QIN-BREAST. Please check the data folder.\n",
      "Already processed TCGA-LGG. Please check the data folder.\n",
      "Already processed TCGA-KIRC. Please check the data folder.\n",
      "Already processed TCGA-LUAD. Please check the data folder.\n",
      "Already processed Prostate-3T. Please check the data folder.\n",
      "Already processed QIN-HEADNECK. Please check the data folder.\n",
      "Already processed TCGA-PRAD. Please check the data folder.\n",
      "Already processed NSCLC Radiogenomics. Please check the data folder.\n",
      "Already processed Head-Neck Cetuximab. Please check the data folder.\n",
      "Already processed TCGA-LIHC. Please check the data folder.\n",
      "Already processed TCGA-KIRP. Please check the data folder.\n",
      "Already processed TCGA-OV. Please check the data folder.\n",
      "Already processed TCGA-HNSC. Please check the data folder.\n",
      "Already processed TCGA-KICH. Please check the data folder.\n",
      "Already processed TCGA-BLCA. Please check the data folder.\n",
      "Already processed TCGA-LUSC. Please check the data folder.\n",
      "Already processed TCGA-COAD. Please check the data folder.\n",
      "Already processed TCGA-THCA. Please check the data folder.\n",
      "Already processed TCGA-READ. Please check the data folder.\n",
      "Already processed NSCLC-Radiomics-Genomics. Please check the data folder.\n",
      "Already processed Lung Phantom. Please check the data folder.\n",
      "Already processed NSCLC-Radiomics. Please check the data folder.\n",
      "Already processed QIN Breast DCE-MRI. Please check the data folder.\n",
      "Already processed QIN PET Phantom. Please check the data folder.\n",
      "Already processed ISPY1. Please check the data folder.\n",
      "Already processed TCGA-UCEC. Please check the data folder.\n",
      "Already processed QIN GBM Treatment Response. Please check the data folder.\n",
      "Already processed SPIE-AAPM Lung CT Challenge. Please check the data folder.\n",
      "Already processed TCGA-ESCA. Please check the data folder.\n",
      "Already processed TCGA-STAD. Please check the data folder.\n",
      "Already processed TCGA-CESC. Please check the data folder.\n",
      "Already processed QIN LUNG CT. Please check the data folder.\n",
      "Already processed TCGA-SARC. Please check the data folder.\n",
      "Already processed LungCT-Diagnosis. Please check the data folder.\n",
      "Already processed CT Lymph Nodes. Please check the data folder.\n",
      "Already processed Mouse-Astrocytoma. Please check the data folder.\n",
      "Already processed Mouse-Mammary. Please check the data folder.\n",
      "Already processed Prostate Fused-MRI-Pathology. Please check the data folder.\n",
      "Already processed Soft-tissue-Sarcoma. Please check the data folder.\n",
      "Already processed 4D-Lung. Please check the data folder.\n",
      "Already processed Breast-MRI-NACT-Pilot. Please check the data folder.\n",
      "Already processed Pancreas-CT. Please check the data folder.\n",
      "Already processed: 58\n",
      "Skipping CBIS-DDSM because of size\n",
      "Already processed PROSTATEx. Please check the data folder.\n",
      "Already processed: 59\n",
      "Skipping HNSCC because of size\n",
      "Already processed IvyGAP. Please check the data folder.\n",
      "Already processed Head-Neck-PET-CT. Please check the data folder.\n",
      "Already processed LCTSC. Please check the data folder.\n",
      "Already processed LGG-1p19qDeletion. Please check the data folder.\n",
      "Already processed CC-Radiomics-Phantom. Please check the data folder.\n",
      "Already processed ACRIN-FLT-Breast. Please check the data folder.\n",
      "Already processed CPTAC-PDA. Please check the data folder.\n",
      "Already processed CPTAC-UCEC. Please check the data folder.\n",
      "Already processed CPTAC-CCRCC. Please check the data folder.\n",
      "Already processed CPTAC-LUAD. Please check the data folder.\n",
      "Already processed CPTAC-GBM. Please check the data folder.\n",
      "Already processed CPTAC-LSCC. Please check the data folder.\n",
      "Already processed CPTAC-CM. Please check the data folder.\n",
      "Already processed Brain-Tumor-Progression. Please check the data folder.\n",
      "Already processed APOLLO. Please check the data folder.\n",
      "Already processed CPTAC-HNSCC. Please check the data folder.\n",
      "Already processed Anti-PD-1_MELANOMA. Please check the data folder.\n",
      "Already processed MRI-DIR. Please check the data folder.\n",
      "Already processed Lung-Fused-CT-Pathology. Please check the data folder.\n",
      "Already processed HNSCC-3DCT-RT. Please check the data folder.\n",
      "Already processed ACRIN-FMISO-Brain. Please check the data folder.\n",
      "Already processed Anti-PD-1_Lung. Please check the data folder.\n",
      "Already processed ACRIN-NSCLC-FDG-PET. Please check the data folder.\n",
      "Already processed LDCT-and-Projection-data. Please check the data folder.\n",
      "Already processed CC-Radiomics-Phantom-2. Please check the data folder.\n",
      "Already processed: 84\n",
      "Skipping VICTRE because of size\n",
      "Already processed CC-Radiomics-Phantom-3. Please check the data folder.\n",
      "Already processed NSCLC-RADIOMICS-INTEROBSERVER1. Please check the data folder.\n",
      "Already processed CPTAC-SAR. Please check the data folder.\n",
      "Already processed AAPM-RT-MAC. Please check the data folder.\n",
      "Already processed C4KC-KiTS. Please check the data folder.\n",
      "Already processed OPC-Radiomics. Please check the data folder.\n",
      "Already processed QIN-BRAIN-DSC-MRI. Please check the data folder.\n",
      "Already processed PDMR-BL0293-F563. Please check the data folder.\n",
      "Already processed Pelvic-Reference-Data. Please check the data folder.\n",
      "Already processed HEAD-NECK-RADIOMICS-HN1. Please check the data folder.\n",
      "Already processed GBM-DSC-MRI-DRO. Please check the data folder.\n",
      "Already processed DRO-Toolkit. Please check the data folder.\n"
     ]
    }
   ],
   "source": [
    "# For each collection, get the Studies, and then the Series\n",
    "# Hypothesis: pandas is far more memory efficient than Python dicts so first go get every single\n",
    "#   Study for all passed in collections and put into a DataFrame and THEN go get Series\n",
    "\n",
    "already_processed_count = 0\n",
    "for collection in tcia_collections: # iter through the dictionaries in the list\n",
    "    \n",
    "    # To save a LOT of time, don't rerun collections if they've already been run\n",
    "\n",
    "    if os.path.exists(f'data/{collection[\"Collection\"]}-series.json'):\n",
    "        print(f'Already processed {collection[\"Collection\"]}. Please check the data folder.')\n",
    "        already_processed_count +=1 \n",
    "        continue\n",
    "        \n",
    "    print(f'Already processed: {already_processed_count}')    \n",
    "    \n",
    "    if collection['Collection'] in ['CBIS-DDSM','VICTRE','HNSCC']: #QIBA CT-1C only 486, others over 7,000, HNSCC is 1200\n",
    "        print(f'Skipping {collection[\"Collection\"]} because of size')\n",
    "        continue\n",
    "    \n",
    "    #NOTE: GBM-DSC-MRI-DRO has ZERO series????\n",
    "    \n",
    "    series_list = [] # list to store all the series for a collection\n",
    "    counter = 0\n",
    "    # Get the Studies\n",
    "    \n",
    "    #with httpx.AsyncClient() as async_client: # This doesn't work with 0.12.1 of HTTPX, must be explicit\n",
    "    async_client = httpx.AsyncClient()\n",
    "    print('getting studies')\n",
    "    studies = await getPatientStudiesPerCollection(collection['Collection'], async_client)\n",
    "    print(f'studies in {collection[\"Collection\"]} : {len(studies)}')\n",
    "    #print(f'{collection[\"Collection\"]}_studies.csv')\n",
    "    study_df = pd.DataFrame.from_dict(studies)\n",
    "    study_df.to_csv(f'{collection[\"Collection\"]}_studies.csv')\n",
    "\n",
    "    # Need to make this MUCH faster\n",
    "#    for study in studies[:3]:\n",
    "#        series = await getSeriesPerStudy(study['StudyInstanceUID'], async_client)\n",
    "#        counter += 1\n",
    "#        if counter % 50 == 0:\n",
    "#            print(str(counter))\n",
    "\n",
    "        #create a list of series dicts (combining metadata from study)\n",
    "#        for s in series:\n",
    "#            # merge the dictionaries using ** to unpack the dictionaries (since .union is in place)\n",
    "#            merged_dict = {**study, **s}\n",
    "#            series_list.append(merged_dict)\n",
    "\n",
    "    # Try this instead\n",
    "    study_ids = [s['StudyInstanceUID'] for s in studies]\n",
    "    batchsize = 10    \n",
    "\n",
    "\n",
    "    for i in range(0,len(study_ids),batchsize):\n",
    "        start = time.time()                  \n",
    "        trimmed_range = [y for y in range(i,i+batchsize) if y<len(study_ids)]   #only used to shorten the last batch    \n",
    "        print(trimmed_range)\n",
    "        #trimmed_study_ids = [study_ids[i] for i in trimmed_range]\n",
    "        #print(trimmed_study_ids)\n",
    "        trimmed_studies = [studies[i] for i in trimmed_range]\n",
    "        #print(trimmed_studies)\n",
    "        #1.3.6.1.4.1.14519.5.2.1.7695.4001.130563880911723253267280582465\n",
    "        responses = await asyncio.gather(*[getSeriesPerStudyJoinDict(s[\"StudyInstanceUID\"],s, async_client) for s in trimmed_studies])\n",
    "        #responses = await asyncio.gather(*[getSeriesPerStudyJoinDict('1.3.6.1.4.1.14519.5.2.1.7695.4001.130563880911723253267280582465',s, async_client) for s in trimmed_studies])\n",
    "        # unfortunately responses is a list of list of dicts, we need to flatten\n",
    "        flatten_matrix = [val for sublist in responses for val in sublist] \n",
    "        #print(len(flatten_matrix))\n",
    "        [series_list.append(r) for r in flatten_matrix]\n",
    "        #print(len(series_list))\n",
    "        print(f'Studies {i}-{i+batchsize} in {time.time() - start} seconds')  \n",
    "    #print(len(series_list))    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Output results to a data folder to avoid having to burn time running again\n",
    "    study_df = pd.DataFrame.from_dict(series_list)\n",
    "    study_df.to_csv(f'data/{collection[\"Collection\"]}_studies_series.csv')\n",
    "\n",
    "    # Also save just the resulting list, since that can be useful, too.  :-)\n",
    "    with open(f'data/{collection[\"Collection\"]}-series.json',\"w\") as f:\n",
    "        json.dump(series_list, f)\n",
    "    \n",
    "    # close the connection explicitly, until with is supported by HTTPX\n",
    "    await async_client.aclose()\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Series Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TCGA-GBM.\n",
      "Processing 6208 series.\n",
      "Series 0-50 in 7.066700220108032 seconds\n",
      "Series 50-100 in 8.26707649230957 seconds\n",
      "Series 100-150 in 3.630408525466919 seconds\n",
      "Series 150-200 in 1.256850242614746 seconds\n",
      "Series 200-250 in 1.352020263671875 seconds\n",
      "Series 250-300 in 1.8067431449890137 seconds\n",
      "Series 300-350 in 1.7508928775787354 seconds\n",
      "Series 350-400 in 1.5195119380950928 seconds\n",
      "Series 400-450 in 1.614271640777588 seconds\n",
      "Series 450-500 in 1.318326473236084 seconds\n",
      "Series 500-550 in 1.5159130096435547 seconds\n",
      "Series 550-600 in 1.4819550514221191 seconds\n",
      "Series 600-650 in 2.27018141746521 seconds\n",
      "Series 650-700 in 2.9213478565216064 seconds\n",
      "Series 700-750 in 1.8629369735717773 seconds\n",
      "Series 750-800 in 2.276095151901245 seconds\n",
      "Series 800-850 in 2.502464532852173 seconds\n",
      "Series 850-900 in 1.7380445003509521 seconds\n",
      "Series 900-950 in 2.0274152755737305 seconds\n",
      "Series 950-1000 in 2.1989922523498535 seconds\n",
      "Series 1000-1050 in 2.890496015548706 seconds\n",
      "Series 1050-1100 in 2.3256428241729736 seconds\n",
      "Series 1100-1150 in 2.444582462310791 seconds\n",
      "Series 1150-1200 in 1.4948101043701172 seconds\n",
      "Series 1200-1250 in 3.1169121265411377 seconds\n",
      "Series 1250-1300 in 4.484037637710571 seconds\n",
      "Series 1300-1350 in 2.5862176418304443 seconds\n",
      "Series 1350-1400 in 3.2342324256896973 seconds\n",
      "Series 1400-1450 in 3.051577568054199 seconds\n",
      "Series 1450-1500 in 4.2552268505096436 seconds\n",
      "Series 1500-1550 in 4.557886838912964 seconds\n",
      "Series 1550-1600 in 2.825444221496582 seconds\n",
      "Series 1600-1650 in 1.534785509109497 seconds\n",
      "Series 1650-1700 in 1.1386334896087646 seconds\n",
      "Series 1700-1750 in 1.2879817485809326 seconds\n",
      "Series 1750-1800 in 1.9357333183288574 seconds\n",
      "Series 1800-1850 in 2.062084436416626 seconds\n",
      "Series 1850-1900 in 1.7037286758422852 seconds\n",
      "Series 1900-1950 in 1.5537996292114258 seconds\n",
      "Series 1950-2000 in 1.5713608264923096 seconds\n",
      "Series 2000-2050 in 2.4519083499908447 seconds\n",
      "Series 2050-2100 in 2.903681516647339 seconds\n",
      "Series 2100-2150 in 3.5568838119506836 seconds\n",
      "Series 2150-2200 in 3.039407253265381 seconds\n",
      "Series 2200-2250 in 3.6162021160125732 seconds\n",
      "Series 2250-2300 in 3.912693500518799 seconds\n",
      "Series 2300-2350 in 4.975989103317261 seconds\n",
      "Series 2350-2400 in 5.654170036315918 seconds\n",
      "Series 2400-2450 in 2.2097129821777344 seconds\n",
      "Series 2450-2500 in 3.2100026607513428 seconds\n",
      "Series 2500-2550 in 2.9276556968688965 seconds\n",
      "Series 2550-2600 in 3.0893688201904297 seconds\n",
      "Series 2600-2650 in 1.8642816543579102 seconds\n",
      "Series 2650-2700 in 2.8198275566101074 seconds\n",
      "Series 2700-2750 in 3.7657532691955566 seconds\n",
      "Series 2750-2800 in 4.343526601791382 seconds\n",
      "Series 2800-2850 in 3.316054582595825 seconds\n",
      "Series 2850-2900 in 4.671928882598877 seconds\n",
      "Series 2900-2950 in 3.875563859939575 seconds\n",
      "Series 2950-3000 in 3.462801456451416 seconds\n",
      "Series 3000-3050 in 4.0068678855896 seconds\n",
      "Series 3050-3100 in 3.3955488204956055 seconds\n",
      "Series 3100-3150 in 3.045074224472046 seconds\n",
      "Series 3150-3200 in 3.231821060180664 seconds\n",
      "Series 3200-3250 in 2.127601385116577 seconds\n",
      "Series 3250-3300 in 1.573737382888794 seconds\n",
      "Series 3300-3350 in 2.020890235900879 seconds\n",
      "Series 3350-3400 in 1.8580858707427979 seconds\n",
      "Series 3400-3450 in 1.7097880840301514 seconds\n",
      "Series 3450-3500 in 1.1474335193634033 seconds\n",
      "Series 3500-3550 in 1.2784907817840576 seconds\n",
      "Series 3550-3600 in 1.2764220237731934 seconds\n",
      "Series 3600-3650 in 1.7869951725006104 seconds\n",
      "Series 3650-3700 in 1.4550504684448242 seconds\n",
      "Series 3700-3750 in 1.7637197971343994 seconds\n",
      "Series 3750-3800 in 1.92026686668396 seconds\n",
      "Series 3800-3850 in 2.3199896812438965 seconds\n",
      "Series 3850-3900 in 1.6741197109222412 seconds\n",
      "Series 3900-3950 in 0.937737226486206 seconds\n",
      "Series 3950-4000 in 1.1179122924804688 seconds\n",
      "Series 4000-4050 in 1.3489713668823242 seconds\n",
      "Series 4050-4100 in 1.4949913024902344 seconds\n",
      "Series 4100-4150 in 1.3050577640533447 seconds\n",
      "Series 4150-4200 in 1.6057159900665283 seconds\n",
      "Series 4200-4250 in 2.010986566543579 seconds\n",
      "Series 4250-4300 in 1.7001285552978516 seconds\n",
      "Series 4300-4350 in 1.6100060939788818 seconds\n",
      "Series 4350-4400 in 1.4134595394134521 seconds\n",
      "Series 4400-4450 in 2.2623097896575928 seconds\n",
      "Series 4450-4500 in 1.6339948177337646 seconds\n",
      "Series 4500-4550 in 1.6967058181762695 seconds\n",
      "Series 4550-4600 in 2.049135684967041 seconds\n",
      "Series 4600-4650 in 2.287586212158203 seconds\n",
      "Series 4650-4700 in 1.8368091583251953 seconds\n",
      "Series 4700-4750 in 1.5601541996002197 seconds\n",
      "Series 4750-4800 in 1.6655898094177246 seconds\n",
      "Series 4800-4850 in 2.038957357406616 seconds\n",
      "Series 4850-4900 in 1.5970070362091064 seconds\n",
      "Series 4900-4950 in 1.633202314376831 seconds\n",
      "Series 4950-5000 in 1.4880759716033936 seconds\n",
      "Series 5000-5050 in 1.55489182472229 seconds\n",
      "Series 5050-5100 in 1.288923978805542 seconds\n",
      "Series 5100-5150 in 1.6358458995819092 seconds\n",
      "Series 5150-5200 in 1.0442206859588623 seconds\n",
      "Series 5200-5250 in 1.4731624126434326 seconds\n",
      "Series 5250-5300 in 3.1169259548187256 seconds\n",
      "Series 5300-5350 in 2.694838285446167 seconds\n",
      "Series 5350-5400 in 1.8050761222839355 seconds\n",
      "Series 5400-5450 in 1.3858025074005127 seconds\n",
      "Series 5450-5500 in 2.3524718284606934 seconds\n",
      "Series 5500-5550 in 3.3367488384246826 seconds\n",
      "Series 5550-5600 in 1.377790927886963 seconds\n",
      "Series 5600-5650 in 1.6310296058654785 seconds\n",
      "Series 5650-5700 in 1.7549335956573486 seconds\n",
      "Series 5700-5750 in 2.787637710571289 seconds\n",
      "Series 5750-5800 in 3.488708734512329 seconds\n",
      "Series 5800-5850 in 2.2164902687072754 seconds\n",
      "Series 5850-5900 in 1.6627166271209717 seconds\n",
      "Series 5900-5950 in 1.8619511127471924 seconds\n",
      "Series 5950-6000 in 1.330976963043213 seconds\n",
      "Series 6000-6050 in 2.548968553543091 seconds\n",
      "Series 6050-6100 in 2.221349000930786 seconds\n",
      "Series 6100-6150 in 3.907752275466919 seconds\n",
      "Series 6150-6200 in 3.287621259689331 seconds\n",
      "Series 6200-6250 in 2.651005268096924 seconds\n",
      "481954\n",
      "Already processed LIDC-IDRI. Please check the data folder.\n",
      "Already processed BREAST-DIAGNOSIS. Please check the data folder.\n",
      "Already processed PROSTATE-MRI. Please check the data folder.\n",
      "Already processed PROSTATE-DIAGNOSIS. Please check the data folder.\n",
      "Already processed NaF PROSTATE. Please check the data folder.\n",
      "Already processed CT COLONOGRAPHY. Please check the data folder.\n",
      "Already processed REMBRANDT. Please check the data folder.\n",
      "Already processed RIDER Breast MRI. Please check the data folder.\n",
      "Already processed RIDER Lung CT. Please check the data folder.\n",
      "Already processed RIDER NEURO MRI. Please check the data folder.\n",
      "Already processed RIDER PHANTOM MRI. Please check the data folder.\n",
      "Already processed RIDER PHANTOM PET-CT. Please check the data folder.\n",
      "Already processed RIDER Lung PET-CT. Please check the data folder.\n",
      "Already processed QIBA CT-1C. Please check the data folder.\n",
      "Already processed Phantom FDA. Please check the data folder.\n",
      "Already processed TCGA-BRCA. Please check the data folder.\n",
      "Already processed QIN-BREAST. Please check the data folder.\n",
      "Already processed TCGA-LGG. Please check the data folder.\n",
      "Already processed TCGA-KIRC. Please check the data folder.\n",
      "Already processed TCGA-LUAD. Please check the data folder.\n",
      "Already processed Prostate-3T. Please check the data folder.\n",
      "Already processed QIN-HEADNECK. Please check the data folder.\n",
      "Already processed TCGA-PRAD. Please check the data folder.\n",
      "Already processed NSCLC Radiogenomics. Please check the data folder.\n",
      "Already processed Head-Neck Cetuximab. Please check the data folder.\n",
      "Already processed TCGA-LIHC. Please check the data folder.\n",
      "Already processed TCGA-KIRP. Please check the data folder.\n",
      "Already processed TCGA-OV. Please check the data folder.\n",
      "Already processed TCGA-HNSC. Please check the data folder.\n",
      "Already processed TCGA-KICH. Please check the data folder.\n",
      "Already processed TCGA-BLCA. Please check the data folder.\n",
      "Already processed TCGA-LUSC. Please check the data folder.\n",
      "Already processed TCGA-COAD. Please check the data folder.\n",
      "Already processed TCGA-THCA. Please check the data folder.\n",
      "Already processed TCGA-READ. Please check the data folder.\n",
      "Already processed NSCLC-Radiomics-Genomics. Please check the data folder.\n",
      "Already processed Lung Phantom. Please check the data folder.\n",
      "Already processed NSCLC-Radiomics. Please check the data folder.\n",
      "Already processed QIN Breast DCE-MRI. Please check the data folder.\n",
      "Already processed QIN PET Phantom. Please check the data folder.\n",
      "Already processed ISPY1. Please check the data folder.\n",
      "Already processed TCGA-UCEC. Please check the data folder.\n",
      "Already processed QIN GBM Treatment Response. Please check the data folder.\n",
      "Already processed SPIE-AAPM Lung CT Challenge. Please check the data folder.\n",
      "Already processed TCGA-ESCA. Please check the data folder.\n",
      "Already processed TCGA-STAD. Please check the data folder.\n",
      "Already processed TCGA-CESC. Please check the data folder.\n",
      "Already processed QIN LUNG CT. Please check the data folder.\n",
      "Already processed TCGA-SARC. Please check the data folder.\n",
      "Already processed LungCT-Diagnosis. Please check the data folder.\n",
      "Already processed CT Lymph Nodes. Please check the data folder.\n",
      "Already processed Mouse-Astrocytoma. Please check the data folder.\n",
      "Already processed Mouse-Mammary. Please check the data folder.\n",
      "Already processed Prostate Fused-MRI-Pathology. Please check the data folder.\n",
      "Already processed Soft-tissue-Sarcoma. Please check the data folder.\n",
      "Already processed 4D-Lung. Please check the data folder.\n",
      "Already processed Breast-MRI-NACT-Pilot. Please check the data folder.\n",
      "Already processed Pancreas-CT. Please check the data folder.\n",
      "Cannot find csv for CBIS-DDSM. Please check the data folder.\n",
      "Already processed PROSTATEx. Please check the data folder.\n",
      "Cannot find csv for HNSCC. Please check the data folder.\n",
      "Already processed IvyGAP. Please check the data folder.\n",
      "Already processed Head-Neck-PET-CT. Please check the data folder.\n",
      "Already processed LCTSC. Please check the data folder.\n",
      "Already processed LGG-1p19qDeletion. Please check the data folder.\n",
      "Already processed CC-Radiomics-Phantom. Please check the data folder.\n",
      "Already processed ACRIN-FLT-Breast. Please check the data folder.\n",
      "Already processed CPTAC-PDA. Please check the data folder.\n",
      "Already processed CPTAC-UCEC. Please check the data folder.\n",
      "Already processed CPTAC-CCRCC. Please check the data folder.\n",
      "Already processed CPTAC-LUAD. Please check the data folder.\n",
      "Already processed CPTAC-GBM. Please check the data folder.\n",
      "Already processed CPTAC-LSCC. Please check the data folder.\n",
      "Already processed CPTAC-CM. Please check the data folder.\n",
      "Already processed Brain-Tumor-Progression. Please check the data folder.\n",
      "Already processed APOLLO. Please check the data folder.\n",
      "Already processed CPTAC-HNSCC. Please check the data folder.\n",
      "Already processed Anti-PD-1_MELANOMA. Please check the data folder.\n",
      "Already processed MRI-DIR. Please check the data folder.\n",
      "Already processed Lung-Fused-CT-Pathology. Please check the data folder.\n",
      "Already processed HNSCC-3DCT-RT. Please check the data folder.\n",
      "Already processed ACRIN-FMISO-Brain. Please check the data folder.\n",
      "Already processed Anti-PD-1_Lung. Please check the data folder.\n",
      "Already processed ACRIN-NSCLC-FDG-PET. Please check the data folder.\n",
      "skipping LDCT and Projection data\n",
      "Already processed CC-Radiomics-Phantom-2. Please check the data folder.\n",
      "Cannot find csv for VICTRE. Please check the data folder.\n",
      "Already processed CC-Radiomics-Phantom-3. Please check the data folder.\n",
      "Already processed NSCLC-RADIOMICS-INTEROBSERVER1. Please check the data folder.\n",
      "Already processed CPTAC-SAR. Please check the data folder.\n",
      "Already processed AAPM-RT-MAC. Please check the data folder.\n",
      "Already processed C4KC-KiTS. Please check the data folder.\n",
      "Already processed OPC-Radiomics. Please check the data folder.\n",
      "Already processed QIN-BRAIN-DSC-MRI. Please check the data folder.\n",
      "Already processed PDMR-BL0293-F563. Please check the data folder.\n",
      "Already processed Pelvic-Reference-Data. Please check the data folder.\n",
      "Already processed HEAD-NECK-RADIOMICS-HN1. Please check the data folder.\n",
      "Already processed GBM-DSC-MRI-DRO. Please check the data folder.\n",
      "Already processed DRO-Toolkit. Please check the data folder.\n"
     ]
    }
   ],
   "source": [
    "## use existing TCIA collections\n",
    "for collection in tcia_collections: # iter through the dictionaries in the list\n",
    "    \n",
    "    # Don't try to do items that aready exist\n",
    "    if not os.path.exists(f'data/{collection[\"Collection\"]}-series.json'):\n",
    "        print(f'Cannot find csv for {collection[\"Collection\"]}. Please check the data folder.')\n",
    "        continue\n",
    "        \n",
    "    if collection['Collection']==\"LDCT-and-Projection-data\":\n",
    "        print('skipping LDCT and Projection data')\n",
    "        continue\n",
    "        \n",
    "    if os.path.exists(f'data/{collection[\"Collection\"]}-instances.json'):\n",
    "        print(f'Already processed {collection[\"Collection\"]}. Please check the data folder.')\n",
    "        continue\n",
    "        \n",
    "    print(f'Processing {collection[\"Collection\"]}.')\n",
    "    with open(f'data/{collection[\"Collection\"]}-series.json', 'r') as myfile:\n",
    "        data = myfile.read()\n",
    "\n",
    "    data = data.replace(\"\\'\", \"\\\"\") # stupidly encoded my JSON with single quotes, instead of double - fix that with hack\n",
    "    series=json.loads(data)\n",
    "    \n",
    "    async_client = httpx.AsyncClient()\n",
    " \n",
    "    instance_list = []    \n",
    "    #series_ids = [s['SeriesInstanceUID'] for s in series]\n",
    "    batchsize = 50    \n",
    "\n",
    "    #print(series_ids[:10])   \n",
    "    #series = series[:60]\n",
    "    print(f'Processing {len(series)} series.')\n",
    "\n",
    "    for i in range(0,len(series),batchsize):               \n",
    "        trimmed_range = [y for y in range(i,i+batchsize) if y<len(series)]   #only used to shorten the last batch    \n",
    "        #print(f'trimmed_range is {trimmed_range}')\n",
    "\n",
    "        trimmed_series = [series[i] for i in trimmed_range]\n",
    "        #print(len(trimmed_series))\n",
    "        start = time.time()   \n",
    "        \n",
    "      \n",
    "        responses = await asyncio.gather(*[getInstancesPerSeriesJoinDict(s[\"SeriesInstanceUID\"],s, async_client) for s in trimmed_series])\n",
    "        print(f'Series {i}-{i+batchsize} in {time.time() - start} seconds')\n",
    "        #print(len(responses))\n",
    "        # unfortunately responses is a list of list of dicts, we need to flatten\n",
    "        flatten_matrix = [val for sublist in responses for val in sublist] \n",
    "        #print(len(flatten_matrix))\n",
    "        [instance_list.append(r) for r in flatten_matrix]\n",
    "    \n",
    "    print(len(instance_list))\n",
    "    \n",
    "\n",
    "\n",
    "    # Output results to a data folder to avoid having to burn time running again\n",
    "    df = pd.DataFrame.from_dict(instance_list)\n",
    "    df.to_csv(f'data/{collection[\"Collection\"]}_studies_series_instances.csv')\n",
    "\n",
    "    # Also save just the resulting list, since that can be useful, too.  :-)\n",
    "    with open(f'data/{collection[\"Collection\"]}-instances.json',\"w\") as f:\n",
    "        json.dump(instance_list, f)\n",
    "    \n",
    "    # close the connection explicitly\n",
    "    await async_client.aclose()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(instance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(study_df))\n",
    "study_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in series_list[:1]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_list2 = list(set(series_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete\n",
    "study_ids = study_ids[0:2]\n",
    "\n",
    "for i in range(0,len(study_ids),batchsize):\n",
    "    start = time.time()                  \n",
    "    trimmed_range = [y for y in range(i,i+batchsize) if y<len(study_ids)]   #only used to shorten the last batch    \n",
    "    print(trimmed_range)\n",
    "    trimmed_study_ids = [study_ids[sid] for sid in trimmed_range]\n",
    "    print(trimmed_study_ids)\n",
    "\n",
    "    responses = await asyncio.gather(*[getSeriesPerStudyJoinDict(sid, async_client) for sid in trimmed_study_ids])\n",
    "    # unfortunately responses is a list of list of dicts, we need to flatten\n",
    "    flatten_matrix = [val for sublist in responses for val in sublist] \n",
    "    #print(len(flatten_matrix))\n",
    "    [series_list.append(r) for r in flatten_matrix]\n",
    "    print(len(series_list))\n",
    "    print(f'Studies {i}-{i+batchsize} in {time.time() - start} seconds')  \n",
    "#print(len(series_list))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    start = time.time()\n",
    "    await asyncio.gather(*[foo() for x in range(i)])\n",
    "    print(f'{i} runs in {time.time() - start} seconds')\n",
    "\n",
    "# Throttle requests to 50 concurrent\n",
    "semaphore = asyncio.Semaphore(50)\n",
    "#...\n",
    "async def limit_wrap(url):\n",
    "    async with semaphore:\n",
    "        # do what you want\n",
    "#...\n",
    "results = asyncio.gather([limit_wrap(url) for url in urls])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_ids = [s['StudyInstanceUID'] for s in studies]\n",
    "stuff = []\n",
    "\n",
    "l = [x for x in range(len(study_ids))]\n",
    "batchsize = 20\n",
    "for x in range(0,len(l),batchsize):\n",
    "    #print('--------')\n",
    "    trimmed = [y for y in range(x,x+batchsize) if y<len(l)]\n",
    "    print(trimmed)\n",
    "\n",
    "    for a in l[x:x+len(trimmed)]:\n",
    "        stuff.append(a)\n",
    "        \n",
    "print(len(stuff))\n",
    "print(len(study_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "study_ids = [s['StudyInstanceUID'] for s in studies]\n",
    "print(len(study_ids))\n",
    "\n",
    "\n",
    "series_list = [] # list to store all the series for a collection\n",
    "\n",
    "entire_start = time.time()\n",
    "async_client = httpx.AsyncClient()\n",
    "batchsize = 10\n",
    "batches=2\n",
    "for i in range(0,batchsize*batches,batchsize):\n",
    "    start = time.time()\n",
    "    responses = await asyncio.gather(*[getSeriesPerStudy(\"1.3.6.1.4.1.14519.5.2.1.7695.4001.130563880911723253267280582465\", async_client) for sid in study_ids[i:i+batchsize]])\n",
    "    # unfortunately responses is a list of list of dicts, we need to flatten\n",
    "    flatten_matrix = [val for sublist in responses for val in sublist] \n",
    "    print(len(flatten_matrix))\n",
    "    [series_list.append(r) for r in flatten_matrix]\n",
    "    print(len(series_list))\n",
    "    print(f'Studies {i}-{i+batchsize} in {time.time() - start} seconds')  \n",
    "await async_client.aclose()\n",
    "print(time.time() - entire_start)\n",
    "print(len(series_list))\n",
    "# 2 batches of 20 studies is 98 seconds, series len = 160\n",
    "# 2 batches of 10 studies is 50 seconds, series len = 80\n",
    "# 2 batches of  5 studius is 26 seconds, series len = 40\n",
    "# 2 batches of  2 studies is 22 seconds, series len = 16\n",
    "# 2 batches of  1 study   is 15 seconds, series len = 8\n",
    "# kind of linear growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(responses))\n",
    "print(len(series_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_matrix = [val for sublist in series_list for val in sublist] \n",
    "print(len(flatten_matrix))\n",
    "\n",
    "series_list[:2]\n",
    "df = pd.DataFrame.from_dict(series_list)\n",
    "print(len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make this MUCH faster\n",
    "\n",
    "\n",
    "\n",
    "for study in studies[:3]:\n",
    "    series = await getSeriesPerStudy(study['StudyInstanceUID'])\n",
    "\n",
    "    #create a list of series dicts (combining metadata from study)\n",
    "    for s in series:\n",
    "        # merge the dictionaries using ** to unpack the dictionaries (since .union is in place)\n",
    "        merged_dict = {**study, **s}\n",
    "        series_list.append(merged_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "collection = tcia_collections[1]\n",
    "with open(f'data/{collection[\"Collection\"]}-series-test.json',\"w\") as f:\n",
    "    json.dump(series_list, f)\n",
    "    #series_list = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(series_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list if we need to\n",
    "collection = tcia_collections[1]\n",
    "with open(f'data/{collection[\"Collection\"]}-series-test.json',\"r\") as f:\n",
    "    l = json.load(f)\n",
    "type(l) # should read 'list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = asq.QueueClient.from_connection_string(conn_str='DefaultEndpointsProtocol=https;AccountName=sjbfunctest;AccountKey=XuYBliYrXazCmfDdK2jLcaJcfqPgu8tC43TlltTMY413nusjx2N6+IvErYmVXuZfOBVgVaCQ52RObKioS9FDRg==;EndpointSuffix=core.windows.net', queue_name='foofoo3')\n",
    "\n",
    "try:\n",
    "    p = q.get_queue_properties()\n",
    "except:\n",
    "    q.create_queue()\n",
    "q.send_message('Hello-There ')\n",
    "r = asq.TextBase64EncodePolicy()\n",
    "r.encode('TEST-THIS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works with the addition of async with \n",
    "\n",
    "import httpx\n",
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "async def download(url:str):\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollections\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        resp = await client.get(url)\n",
    "    return resp\n",
    "\n",
    "async def download_lots(i):\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollections\"\n",
    "    await asyncio.gather(*[download(url) for x in range(i)])\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    asyncio.run(download_lots))  # used outside of Jupyter when I don't have an event loop\n",
    "\n",
    "for i in range(7):\n",
    "    start = time.time()\n",
    "    await download_lots(i)\n",
    "    print(f'{i} runs in {time.time() - start} seconds')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "import os\n",
    "\n",
    "async def download(url:str, folder:str):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    resp = await httpx.get(url)\n",
    "    resp.raise_for_status()\n",
    "    #async with aiofiles.open(os.path.join(folder, filename), \"wb\") as f:\n",
    "    #    await f.write(resp.content)\n",
    "\n",
    "\n",
    "async def download_all_photos(loops: str):\n",
    "    #resp = httpx.get(\"https://jsonplaceholder.typicode.com/photos\")\n",
    "    #resp.raise_for_status()\n",
    "    #urls = list(set(d[\"url\"] for d in resp.json()))[:10]\n",
    "    #os.makedirs(out_dir, exist_ok=True)\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollectionValues\"\n",
    "    await asyncio.gather(*[download(url, \"bob\") for x in range(loops)])\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    asyncio.run(download_all_photos('100_photos'))\n",
    "\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    await download_all_photos(i)\n",
    "    print(f'{i} runs in {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with httpx.AsyncClient() as client:\n",
    "    r = await client.get('https://www.example.com/')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  This works with HTTPX\n",
    "async def foo():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        url = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollectionValues'\n",
    "        r = await client.get(url) #'https://www.example.com/')\n",
    "    return r\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    await asyncio.gather(*[foo() for x in range(i)])\n",
    "    print(f'{i} runs in {time.time() - start} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "from aiohttp import ClientSession\n",
    "import asyncio\n",
    "\n",
    "async def call_url(x, session):\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSeries?Collection=TCGA-GBM&StudyInstanceUID=1.3.6.1.4.1.14519.5.2.1.7695.4001.130563880911723253267280582465\"\n",
    "    \n",
    "    response = await session.get(url, timeout=None)\n",
    "    response_json = await response.json()\n",
    "    return response_json\n",
    "\n",
    "\n",
    "async def run_program(x, session):\n",
    "    \"\"\"Wrapper for running program in an asynchronous manner\"\"\"\n",
    "    #try:\n",
    "    response = await call_url(x, session)\n",
    "        #print(f\"Response: {json.dumps(response, indent=2)}\")\n",
    "    #except Exception as err:\n",
    "        #print(f\"Exception occured: {err}\")\n",
    "        #pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    #async with httpx.AsyncClient() as session:\n",
    "    async with ClientSession as sesssion:\n",
    "        await asyncio.gather(*[run_program(x,session) for x in range(i)])\n",
    "   # print(f'{i} runs in {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    try:\n",
    "        response = await session.request(method='GET', url=url)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Response status ({url}): {response.status}\")\n",
    "    except HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An error ocurred: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works with aiohttp but not httpx \n",
    "import aiohttp\n",
    "import asyncio\n",
    "import time\n",
    "import httpx\n",
    "\n",
    "async def call_url(session):\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollections\"\n",
    "    #response = await session.request(method='GET', url=url)\n",
    "    response = await session.get(url=url)\n",
    "\n",
    "    return response\n",
    "\n",
    "for i in range(1,5):\n",
    "    start = time.time() # start time for timing event\n",
    "    async with aiohttp.ClientSession() as session: #use aiohttp\n",
    "    #async with httpx.AsyncClient as session:  #use httpx\n",
    "        await asyncio.gather(*[call_url(session) for x in range(i)])\n",
    "    print(f'{i} call(s) in {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import time\n",
    "import httpx\n",
    "\n",
    "async def call_url(session):\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollectionValues\"\n",
    "    #response = await session.request(method='GET', url=url)\n",
    "    response = await session.get(url=url)\n",
    "\n",
    "    return response\n",
    "\n",
    "for i in range(1,5):\n",
    "    start = time.time() # start time for timing event\n",
    "    #async with aiohttp.ClientSession() as session: #use aiohttp\n",
    "    session = httpx.AsyncClient() #use httpx\n",
    "    await asyncio.gather(*[call_url(session) for x in range(i)])\n",
    "    await session.aclose()\n",
    "    print(f'{i} call(s) in {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    start = time.time()\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "    #async with httpx.AsyncClient as session:\n",
    "        await asyncio.gather(*[call_url(session) for x in range(i)])\n",
    "    print(f'{i} call(s) in {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import time\n",
    "import httpx\n",
    "\n",
    "async def call_url(session):\n",
    "    url = \"https://services.cancerimagingarchive.net/services/v3/TCIA/query/getCollections\"\n",
    "    #async with aiohttp.ClientSession() as session: #use aiohttp\n",
    "    async with httpx.AsyncClient as session:  #use httpx\n",
    "        response = await session.get(url=url)\n",
    "\n",
    "    return response\n",
    "\n",
    "for i in range(1,5):\n",
    "    start = time.time() # start time for timing event\n",
    "    await asyncio.gather(*[call_url(session) for x in range(i)])\n",
    "    print(f'{i} call(s) in {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(series_list))\n",
    "print(series_list[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_df = pd.DataFrame.from_dict(series_list)\n",
    "study_df.to_csv(f'data/{tcia_collections[0][\"Collection\"]}_studies_series.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(study_df)\n",
    "with open('data/TCGA-GBM-series.json',\"w\") as f:\n",
    "    f.write(str(series_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_sample = series[0]    \n",
    "study_sample = studies[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "series_fields = [x for x in series_sample]\n",
    "study_fields = [x for x in study_sample]\n",
    "print(len(series_fields))\n",
    "print(len(study_fields))\n",
    "merged = list(set(series_fields).union(set(study_fields)))\n",
    "\n",
    "merged2 = {**study_sample, **series_sample}\n",
    "\n",
    "print(study_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sample.update(series_sample)\n",
    "print(len(study_sample))\n",
    "print(study_sample)\n",
    "\n",
    "merged2 = {**study_sample, **series_sample}\n",
    "print(len(merged2))\n",
    "print(merged2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sample.update(series_sample)\n",
    "print(len(study_sample))\n",
    "study_sample\n",
    "from collections import OrderedDict\n",
    "od = OrderedDict(study_sample)\n",
    "od\n",
    "\n",
    "x = []\n",
    "x.append(study_sample)\n",
    "\n",
    "df = pd.DataFrame.from_dict(x)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(series_sample)\n",
    "print(study_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSeries?Collection=TCGA-GBM&StudyInstanceUID=1.3.6.1.4.1.14519.5.2.1.7695.4001.130563880911723253267280582465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studies = [s['StudyInstanceUID'] for s in res.json()]\n",
    "#for x in res.json():\n",
    "#    print(x['StudyInstanceUID'])\n",
    "studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for x in collections.json()[:2]:\n",
    "        print(x['Collection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with all the studies\n",
    "studies = [s['StudyInstanceUID'] for s in res.json()]\n",
    "\n",
    "\n",
    "#storageConnString = os.environ[\"AzureWebJobsStorage\"]\n",
    "storageConnString = 'DefaultEndpointsProtocol=https;AccountName=sjbfunctest;AccountKey=XuYBliYrXazCmfDdK2jLcaJcfqPgu8tC43TlltTMY413nusjx2N6+IvErYmVXuZfOBVgVaCQ52RObKioS9FDRg==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "#x = asq.QueueService(account_name='sjbfunctest', account_key='mykey')\n",
    "#service = asq.QueueServiceClient.from_connection_string(conn_str=connection_string)\n",
    "patient_studies_queue = asq.QueueClient.from_connection_string(conn_str=storageConnString,queue_name='studies')\n",
    "\n",
    "# Create the queue if it doesn't exist...  by exception\n",
    "#   Which is hacky, but effective\n",
    "try:\n",
    "    patient_studies_queue.get_queue_properties()\n",
    "except:\n",
    "    patient_studies_queue.create_queue()\n",
    "\n",
    "# Must base-64 encode since... functions...\n",
    "enc = asq.TextBase64EncodePolicy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for study in studies:\n",
    "    b64 = enc.encode(study)\n",
    "    patient_studies_queue.send_message(b64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(studies)\n",
    "study_id = studies[0]\n",
    "study_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_id = studies[0]\n",
    "\n",
    "urlGetSeries = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getSeries'\n",
    "params = {'StudyInstanceUID': study_id}\n",
    "res = requests.get(urlGetSeries,params=params,timeout=None) #timeout=15.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.json()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with all the studies\n",
    "series = [s['SeriesInstanceUID'] for s in res.json()]\n",
    "\n",
    "series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storageConnString = os.environ[\"AzureWebJobsStorage\"]\n",
    "storageConnString = 'DefaultEndpointsProtocol=https;AccountName=sjbfunctest;AccountKey=XuYBliYrXazCmfDdK2jLcaJcfqPgu8tC43TlltTMY413nusjx2N6+IvErYmVXuZfOBVgVaCQ52RObKioS9FDRg==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "series_queue = asq.QueueClient.from_connection_string(conn_str=storageConnString,queue_name='series')\n",
    "\n",
    "# Create the queue if it doesn't exist...  by exception\n",
    "#   Which is hacky, but effective\n",
    "try:\n",
    "    series_queue.get_queue_properties()\n",
    "except:\n",
    "    series_queue.create_queue()\n",
    "\n",
    "# Must base-64 encode since... functions...\n",
    "enc = asq.TextBase64EncodePolicy()\n",
    "\n",
    "for s in series[:1]:\n",
    "    b64 = enc.encode(s)\n",
    "    series_queue.send_message(b64)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the zip files\n",
    "url = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getImage?SeriesInstanceUID=1.3.6.1.4.1.14519.5.2.1.7695.4001.306204232344341694648035234440'\n",
    "res = requests.get(url,timeout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Get the study id from the base-64 encoded incoming queue\n",
    "#series_id = msg.get_body().decode('utf-8')\n",
    "series_id = series[0]\n",
    "\n",
    "series_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlGetImage = 'https://services.cancerimagingarchive.net/services/v3/TCIA/query/getImage'\n",
    "params = {'SeriesInstanceUID': series_id}\n",
    "res = requests.get(urlGetImage,params=params,timeout=None) #timeout=15.0)\n",
    "print(res.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "\n",
    "def download_file(url):\n",
    "    local_filename = \"foo4.zip\"\n",
    "    with requests.get(urlGetImage, stream=True) as r:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_filename = \"food4.zip\"\n",
    "with requests.get(urlGetImage,params=params,timeout=None, stream=True) as r:\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        shutil.copyfileobj(r.raw, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "file_like_object = io.BytesIO(res.content)\n",
    "z = zipfile.ZipFile(file_like_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = z.filelist\n",
    "f1 = files[1]\n",
    "\n",
    "#for f in files:\n",
    "    #print(f)\n",
    "    #z.read(f)\n",
    "dcmbytes = z.read(f)\n",
    "#dcmbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install azure.storage.blob\n",
    "\n",
    "import azure.storage.blob as blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = f1.filename.split('/')\n",
    "dcm_names = [p for p in parts if p.find('.dcm') != -1]\n",
    "if len(dcm_names) > 0:\n",
    "    dcm_name = dcm_names[0]\n",
    "dcm_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageConnString = 'DefaultEndpointsProtocol=https;AccountName=sjbfunctest;AccountKey=XuYBliYrXazCmfDdK2jLcaJcfqPgu8tC43TlltTMY413nusjx2N6+IvErYmVXuZfOBVgVaCQ52RObKioS9FDRg==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "b = blob.ContainerClient.from_connection_string(conn_str=storageConnString,container_name='dicoms2')\n",
    "\n",
    "# Create the queue if it doesn't exist...  by exception\n",
    "#   Which is hacky, but effective\n",
    "try:\n",
    "    b.get_container_properties()\n",
    "except:\n",
    "    b.create_container()\n",
    "\n",
    "for f in files:\n",
    "    dicom_file = z.read(f)\n",
    "    parts = f.filename.split('/')\n",
    "    dcm_parts = [p for p in parts if p.find('.dcm') != -1]\n",
    "    if len(dcm_parts) == 1: # we have a dicom file, and only one\n",
    "        dcm_name = f'{series_id}/{dcm_parts[0]}'\n",
    "        print(dcm_name)\n",
    "        up = b.upload_blob(data=z.read(f), name=dcm_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#upblob = b.upload_blob(data=dcmbytes,name='test3.dcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upblob.blob_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "storageConnString = 'DefaultEndpointsProtocol=https;AccountName=sjbfunctest;AccountKey=XuYBliYrXazCmfDdK2jLcaJcfqPgu8tC43TlltTMY413nusjx2N6+IvErYmVXuZfOBVgVaCQ52RObKioS9FDRg==;EndpointSuffix=core.windows.net'\n",
    "\n",
    "series_queue = asq.QueueClient.from_connection_string(conn_str=storageConnString,queue_name='series')\n",
    "\n",
    "# Create the queue if it doesn't exist...  by exception\n",
    "#   Which is hacky, but effective\n",
    "try:\n",
    "    series_queue.get_queue_properties()\n",
    "except:\n",
    "    series_queue.create_queue()\n",
    "\n",
    "# Must base-64 encode since... functions...\n",
    "enc = asq.TextBase64EncodePolicy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create a list with all the studies\n",
    "    series = [s['SeriesInstanceUID'] for s in res.json()]\n",
    "\n",
    "\n",
    "    storageConnString = os.environ[\"AzureWebJobsStorage\"]\n",
    "  \n",
    "    series_queue = asq.QueueClient.from_connection_string(conn_str=storageConnString,queue_name='series')\n",
    "\n",
    "    # Create the queue if it doesn't exist...  by exception\n",
    "    #   Which is hacky, but effective\n",
    "    try:\n",
    "        series_queue.get_queue_properties()\n",
    "    except:\n",
    "        series_queue.create_queue()\n",
    "\n",
    "    # Must base-64 encode since... functions...\n",
    "    enc = asq.TextBase64EncodePolicy()\n",
    "\n",
    "    for s in series:\n",
    "        b64 = enc.encode(s)\n",
    "        series_queue.send_message(b64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "import os\n",
    "\n",
    "async def download(url:str, folder:str):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    resp = await httpx.get(url)\n",
    "    resp.raise_for_status()\n",
    "    async with aiofiles.open(os.path.join(folder, filename), \"wb\") as f:\n",
    "        await f.write(resp.content)\n",
    "\n",
    "\n",
    "async def download_all_photos(out_dir: str):\n",
    "    resp = await httpx.get(\"https://jsonplaceholder.typicode.com/photos\")\n",
    "    resp.raise_for_status()\n",
    "    urls = list(set(d[\"url\"] for d in resp.json()))[:100]\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    await asyncio.gather(*[download(url, out_dir) for url in urls])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(download_all_photos('100_photos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WORKS\n",
    "\n",
    "import httpx\n",
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "import os\n",
    "\n",
    "async def download(url:str, folder:str):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    async with httpx.AsyncClient() as session:\n",
    "        resp = await session.get(url)\n",
    "        resp.raise_for_status()\n",
    "    async with aiofiles.open(os.path.join(folder, filename), \"wb\") as f:\n",
    "        await f.write(resp.content)\n",
    "        \n",
    "async def download_all_photos(out_dir: str):\n",
    "    async with httpx.AsyncClient() as session:\n",
    "        resp = await session.get(\"https://jsonplaceholder.typicode.com/photos\")\n",
    "        resp.raise_for_status()\n",
    "    urls = list(set(d[\"url\"] for d in resp.json()))[:100]\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    await asyncio.gather(*[download(url, out_dir) for url in urls])\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    asyncio.run(download_all_photos('100_photos'))\n",
    "    \n",
    "await download_all_photos('100_photos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stuff(f):\n",
    "    f.write('And stuff with context passed to another method. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stuff(f):\n",
    "    f.write('And stuff with context passed to another method. ')\n",
    "    \n",
    "with open('foo.txt',\"w\") as f:\n",
    "    f.write('Start with context manager inside with statement. ')\n",
    "    write_stuff(f)\n",
    "    f.write('And back to close the with.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show httpx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
